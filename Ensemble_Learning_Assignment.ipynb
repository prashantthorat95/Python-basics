{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Ensemble Learning"
      ],
      "metadata": {
        "id": "vGuPaPx5OZ2o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Can we use Bagging for regression problems?**\n",
        "\n",
        "### ‚úÖ Answer:\n",
        "Yes, **Bagging (Bootstrap Aggregating)** can be used for **regression problems**. When applied to regression, it is called a **Bagging Regressor**.\n",
        "\n",
        "### üîç Conceptual Explanation:\n",
        "- Bagging creates **multiple subsets** of the original dataset using **bootstrap sampling** (sampling with replacement).\n",
        "- A **regression model** (usually a Decision Tree Regressor) is trained on each subset.\n",
        "- Final prediction is obtained by **averaging** predictions from all models.\n",
        "\n",
        "### üßÆ Mathematical Representation:\n",
        "Let:\n",
        "- \\( f_1(x), f_2(x), ..., f_n(x) \\) be predictions from \\( n \\) models  \n",
        "\n",
        "Final prediction:\n",
        "\\[\n",
        "\\hat{y} = \\frac{1}{n} \\sum_{i=1}^{n} f_i(x)\n",
        "\\]\n",
        "\n",
        "### üìå Assumptions:\n",
        "- Base models are **high variance models**\n",
        "- Training data is sufficient for resampling\n",
        "\n",
        "### ‚úÖ Advantages:\n",
        "- Reduces **variance**\n",
        "- Improves **stability**\n",
        "- Works well with noisy data\n",
        "\n",
        "### ‚ùå Disadvantages:\n",
        "- Does not reduce **bias**\n",
        "- Increased computational cost\n",
        "\n",
        "### üéØ Use Cases:\n",
        "- House price prediction\n",
        "- Demand forecasting\n",
        "- Stock price regression\n"
      ],
      "metadata": {
        "id": "aBAE7KvhOkrz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l90b2IXpOUda"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. What is the difference between multiple model training and single model training?**\n",
        "\n",
        "### üîç Conceptual Explanation:\n",
        "\n",
        "| Aspect | Single Model Training | Multiple Model Training |\n",
        "|------|----------------------|------------------------|\n",
        "| Models | One model | Multiple models |\n",
        "| Variance | High | Reduced |\n",
        "| Overfitting | More likely | Less likely |\n",
        "| Accuracy | Limited | Higher |\n",
        "| Robustness | Low | High |\n",
        "\n",
        "### üìå Key Difference:\n",
        "- **Single model** learns once from entire data\n",
        "- **Multiple models** learn from different subsets or perspectives\n",
        "\n",
        "### üß† Interview Point:\n",
        "Ensemble methods rely on **multiple model training** to achieve better generalization.\n"
      ],
      "metadata": {
        "id": "WifFVXg_OvaY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QsYcSXW8OYv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Explain the concept of feature randomness in Random Forest**\n",
        "\n",
        "### üîç Concept:\n",
        "Feature randomness means **selecting a random subset of features** at each split while building decision trees.\n",
        "\n",
        "### üß† Why it matters:\n",
        "- Prevents trees from becoming similar\n",
        "- Reduces correlation among trees\n",
        "- Improves model diversity\n",
        "\n",
        "### üßÆ Example:\n",
        "If total features = 10  \n",
        "Random Forest might choose only ‚àö10 ‚âà 3 features at each split.\n",
        "\n",
        "### üìå Advantages:\n",
        "- Better generalization\n",
        "- Less overfitting\n",
        "\n",
        "### üéØ Use Case:\n",
        "High-dimensional datasets like text or bioinformatics\n"
      ],
      "metadata": {
        "id": "Rf-RUA4fPAdN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yXUFL-zjOYsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. What is OOB (Out-of-Bag) Score?**\n",
        "\n",
        "### üîç Concept:\n",
        "OOB Score is an **internal validation technique** used in Bagging and Random Forest.\n",
        "\n",
        "### üß† Explanation:\n",
        "- Each model is trained on ~63% of data\n",
        "- Remaining ~37% is **Out-of-Bag**\n",
        "- OOB samples are used for testing\n",
        "\n",
        "### üßÆ Formula:\n",
        "\\[\n",
        "OOB\\ Error = \\frac{1}{n} \\sum (y - \\hat{y}_{OOB})^2\n",
        "\\]\n",
        "\n",
        "### ‚úÖ Advantages:\n",
        "- No need for separate validation set\n",
        "- Faster evaluation\n",
        "\n",
        "### üéØ Use Case:\n",
        "Random Forest performance evaluation\n"
      ],
      "metadata": {
        "id": "zria6cpfPDuB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpSqhmP2OYnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5.How can you measure the importance of features in a Random Forest model?**\n",
        "\n",
        "Feature importance in a Random Forest model indicates how much each input feature contributes to the model‚Äôs predictions. It can be measured using several methods:\n",
        "\n",
        "**1. Mean Decrease in Impurity (Gini Importance):**  \n",
        "Random Forest builds multiple decision trees, where each split reduces impurity (Gini, Entropy, or MSE). The importance of a feature is calculated by summing the total impurity reduction it produces across all trees and normalizing the result. Features that create large impurity reductions are considered more important. This method is fast but biased toward features with many unique values.\n",
        "\n",
        "**2. Permutation Importance (Mean Decrease in Accuracy):**  \n",
        "After training the model, the values of one feature are randomly shuffled while keeping others unchanged. The drop in model performance (accuracy, RMSE, etc.) is measured. A large performance drop indicates high feature importance. This method is more reliable and model-agnostic but computationally expensive.\n",
        "\n",
        "**3. Out-of-Bag (OOB) Feature Importance:**  \n",
        "Using bootstrap sampling, Random Forest leaves out about 36% of data as OOB samples. Feature values are permuted in the OOB data, and the decrease in performance is measured. A larger drop means higher importance. This method avoids the need for a separate validation set.\n",
        "\n",
        "**4. Split Frequency:**  \n",
        "Feature importance is estimated by counting how often a feature is used to split nodes across all trees. More frequent usage implies higher importance, but this method ignores split quality.\n",
        "\n",
        "**5. SHAP Values (Advanced):**  \n",
        "SHAP assigns feature importance based on game theory by measuring each feature‚Äôs contribution to predictions. It provides both global and local explanations but is computationally expensive.\n",
        "\n",
        "In practice, impurity-based importance is useful for quick insights, while permutation or SHAP-based methods provide more reliable and interpretable results.\n"
      ],
      "metadata": {
        "id": "V9uIMi-_M5sO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZCAuc0uOYkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Explain the working principle of a Bagging Classifier**\n",
        "A Bagging Classifier (Bootstrap Aggregating Classifier) is an ensemble learning technique designed to improve the accuracy and stability of machine learning models by reducing variance.\n",
        "\n",
        "The process begins with **bootstrap sampling**, where multiple training datasets are generated from the original dataset by random sampling **with replacement**. Each bootstrap dataset is the same size as the original dataset but contains repeated samples and omits some original observations.\n",
        "\n",
        "A **base classifier** (commonly a Decision Tree) is then trained independently on each bootstrap dataset. Because each model is trained on a different subset of data, they learn different patterns and decision boundaries.\n",
        "\n",
        "Once all base classifiers are trained, predictions are made for new data points. In classification tasks, the final output is determined using **majority voting**, where the class predicted by the highest number of models becomes the final prediction.\n",
        "\n",
        "Bagging is particularly effective for **high-variance models**, as combining multiple models helps reduce overfitting while maintaining predictive power. This leads to improved generalization and more robust performance on unseen data.\n",
        "\n",
        "### üîç Working Steps:\n",
        "1. Create multiple bootstrap samples\n",
        "2. Train a classifier on each sample\n",
        "3. Combine predictions using **majority voting**\n",
        "\n",
        "### üßÆ Voting Formula:\n",
        "\\[\n",
        "\\hat{y} = \\text{mode}(f_1(x), f_2(x), ..., f_n(x))\n",
        "\\]\n",
        "\n",
        "### üìå Assumptions:\n",
        "- Base models are unstable\n",
        "\n",
        "### üéØ Use Case:\n",
        "Spam detection, fraud detection\n",
        "\n"
      ],
      "metadata": {
        "id": "SWQ2GDAgPd76"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q_D2Zn5POYix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. How do you evaluate a Bagging Classifier‚Äôs performance?**\n",
        "\n",
        "Evaluating a Bagging Classifier involves measuring how well the ensemble model performs on unseen data using appropriate metrics and validation techniques. The evaluation process can be done through the following methods:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Train‚ÄìTest Split Evaluation  \n",
        "The dataset is divided into training and testing sets. The Bagging Classifier is trained on the training set and evaluated on the test set to measure how well it generalizes to new data.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Accuracy Score  \n",
        "Accuracy measures the proportion of correctly classified instances out of the total predictions. It is suitable for balanced classification problems.\n",
        "\n",
        "**Formula:**  \n",
        "Accuracy = (Correct Predictions) / (Total Predictions)\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Confusion Matrix  \n",
        "A confusion matrix provides a detailed breakdown of predictions into:\n",
        "- True Positives\n",
        "- True Negatives\n",
        "- False Positives\n",
        "- False Negatives  \n",
        "\n",
        "This helps in understanding the types of errors made by the Bagging Classifier.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Precision, Recall, and F1-Score  \n",
        "These metrics are important for imbalanced datasets:\n",
        "- **Precision** measures how many predicted positives are correct.\n",
        "- **Recall** measures how many actual positives are correctly identified.\n",
        "- **F1-score** balances precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. ROC Curve and AUC Score  \n",
        "The ROC curve plots True Positive Rate vs False Positive Rate at different thresholds.  \n",
        "The **AUC (Area Under the Curve)** summarizes the model‚Äôs ability to distinguish between classes. Higher AUC indicates better performance.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Cross-Validation  \n",
        "K-fold cross-validation evaluates the Bagging Classifier across multiple data splits, providing a more reliable estimate of performance and reducing dependency on a single train‚Äìtest split.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Out-of-Bag (OOB) Error (If Enabled)  \n",
        "Bagging uses bootstrap sampling, leaving some samples unused for training. These **Out-of-Bag samples** act as a validation set to estimate model performance without requiring a separate test dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "A Bagging Classifier‚Äôs performance is evaluated using metrics such as accuracy, precision, recall, F1-score, ROC-AUC, cross-validation, and OOB error. Using multiple evaluation methods ensures a reliable and well-rounded assessment of the model‚Äôs effectiveness.\n"
      ],
      "metadata": {
        "id": "iWfgHm90PgPD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gSxDAzEOOYgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. How does a Bagging Regressor work?**\n",
        "\n",
        "\n",
        "A Bagging Regressor (Bootstrap Aggregating Regressor) is an ensemble learning method used for regression problems. It improves prediction accuracy and stability by combining the outputs of multiple regression models trained on different subsets of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Working of a Bagging Regressor\n",
        "\n",
        "**1. Bootstrap Sampling**  \n",
        "From the original training dataset, multiple new datasets are created using **random sampling with replacement**. Each bootstrap dataset has the same size as the original dataset but contains duplicate samples and leaves out some observations.\n",
        "\n",
        "**2. Training Multiple Base Regressors**  \n",
        "A separate base regressor (such as Decision Tree Regressor, Linear Regression, or KNN Regressor) is trained on each bootstrap dataset. Since each model sees a different subset of data, they learn different patterns.\n",
        "\n",
        "**3. Independent Learning**  \n",
        "All base regressors are trained independently and in parallel. There is no dependency between models.\n",
        "\n",
        "**4. Making Predictions**  \n",
        "For a new input, each trained regressor produces its own predicted value.\n",
        "\n",
        "**5. Aggregation by Averaging**  \n",
        "The final prediction of the Bagging Regressor is obtained by **averaging the predictions** of all individual regressors. This averaging smooths out errors made by individual models.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Bagging Regressor Is Effective\n",
        "\n",
        "- Reduces variance in high-variance models\n",
        "- Prevents overfitting\n",
        "- Improves generalization on unseen data\n",
        "- Produces more stable predictions\n",
        "\n",
        "---\n",
        "\n",
        "### Example (Conceptual)\n",
        "\n",
        "If 5 regressors predict:\n",
        "- 100, 105, 98, 102, 101  \n",
        "\n",
        "Final Prediction = (100 + 105 + 98 + 102 + 101) / 5 = **101.2**\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "A Bagging Regressor works by training multiple regression models on different bootstrapped datasets and combining their predictions through averaging. This reduces variance, minimizes overfitting, and improves overall predictive performance."
      ],
      "metadata": {
        "id": "Fj9EOkQXPhWa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2HN5CwE1OYd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. What is the main advantage of ensemble techniques?**\n",
        "\n",
        "### ‚úÖ Main Advantage:\n",
        "They **improve model accuracy and robustness** by combining multiple learners.\n",
        "\n",
        "### üìå Key Benefit:\n",
        "- Reduced variance\n",
        "- Better generalization\n",
        "\n",
        "The main advantage of ensemble techniques is that they **improve model accuracy, stability, and generalization** by combining multiple models instead of relying on a single model. This happens through the following step-by-step process:\n",
        "\n",
        "**Step 1: Training Multiple Models**  \n",
        "Ensemble techniques train several base learners (weak or strong models) on the same dataset or on different subsets of the dataset. These models may be of the same type (e.g., decision trees in Random Forest) or different types (e.g., decision tree, SVM, logistic regression).\n",
        "\n",
        "**Step 2: Introducing Diversity**  \n",
        "Each model learns different patterns by using:\n",
        "- Different training samples (Bagging)\n",
        "- Different feature subsets (Random Forest)\n",
        "- Different model weights (Boosting)  \n",
        "This diversity ensures that models make different errors.\n",
        "\n",
        "**Step 3: Individual Predictions**  \n",
        "Each trained model makes its own prediction on new, unseen data. Because models are diverse, their predictions vary.\n",
        "\n",
        "**Step 4: Combining Predictions**  \n",
        "Predictions are combined using techniques such as:\n",
        "- **Majority voting** (classification)\n",
        "- **Averaging** (regression)\n",
        "- **Weighted voting** (boosting methods)\n",
        "\n",
        "**Step 5: Error Reduction**  \n",
        "When predictions are combined:\n",
        "- Random errors cancel out\n",
        "- Variance is reduced\n",
        "- Overfitting is minimized\n",
        "- Overall prediction becomes more reliable\n",
        "\n",
        "**Step 6: Improved Generalization**  \n",
        "The final ensemble model performs better on unseen data compared to individual models, leading to higher accuracy and robustness.\n",
        "\n",
        "**Conclusion:**  \n",
        "Ensemble techniques outperform single models by leveraging multiple diverse learners, reducing bias and variance, and producing more stable and accurate predictions."
      ],
      "metadata": {
        "id": "Gb9xabqhPjnd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pt_P1itvOYbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **10. What is the main challenge of ensemble methods?**\n",
        "\n",
        "The main challenge of ensemble methods is their **increased complexity and computational cost** compared to single models.\n",
        "\n",
        "Ensemble techniques require training and maintaining multiple models instead of just one. This leads to higher **computational resources**, increased **training time**, and greater **memory usage**. As the number of models grows, prediction time can also increase, making ensembles less suitable for real-time or resource-constrained applications.\n",
        "\n",
        "Another challenge is **model interpretability**. Since ensemble methods combine the decisions of many models, understanding how the final prediction is made becomes difficult. This makes debugging, explaining results, and gaining insights from the model harder compared to simpler models like linear regression or single decision trees.\n",
        "\n",
        "Additionally, improper ensemble design can lead to **diminishing returns**, where adding more models does not significantly improve performance and may even introduce noise.\n",
        "\n",
        "**Conclusion:**  \n",
        "While ensemble methods improve accuracy and robustness, their main challenges are higher computational cost, increased complexity, and reduced interpretability.\n",
        "\n",
        "### ‚ùå Challenges:\n",
        "- Increased computation\n",
        "- Reduced interpretability\n",
        "- Complex hyperparameter tuning\n"
      ],
      "metadata": {
        "id": "06IoMh1bPmj4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fTYC9vT7OYY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **11. Explain the key idea behind ensemble techniques**\n",
        "\n",
        "### üîë Core Idea:\n",
        "\"Many weak learners together make a strong learner.\"\n",
        "\n",
        "### üß† Philosophy:\n",
        "Diversity + Aggregation = Better performance\n",
        "\n",
        "\n",
        "\n",
        "The key idea behind ensemble techniques is to **improve predictive performance, stability, and generalization by combining multiple individual models into a single powerful model**. Instead of depending on one model, ensemble methods rely on the collective intelligence of many models.\n",
        "\n",
        "The process begins by training **multiple base learners**. These learners can be of the same type (such as decision trees in Random Forest) or different types (such as decision trees, SVMs, and logistic regression). Each model learns patterns from the data independently.\n",
        "\n",
        "To ensure effectiveness, ensemble techniques introduce **diversity among models**. Diversity is created by using different subsets of training data (bagging), changing feature subsets (Random Forest), assigning different importance to samples (boosting), or using different algorithms (stacking). This diversity ensures that models do not make the same mistakes.\n",
        "\n",
        "After training, each base model produces its own prediction for a given input. Since models are diverse, their predictions vary. These individual predictions are then **combined using aggregation methods** such as majority voting (classification), averaging (regression), or weighted combinations (boosting).\n",
        "\n",
        "By combining predictions, ensemble techniques **reduce variance** (by averaging noisy models), **reduce bias** (by focusing on difficult samples), and **minimize overfitting**. Random errors made by individual models cancel out, while correct predictions reinforce each other.\n",
        "\n",
        "As a result, ensemble techniques produce models that are **more accurate, robust, and reliable**, especially when dealing with complex datasets or high-variance algorithms.\n",
        "\n",
        "**Conclusion:**  \n",
        "The fundamental idea behind ensemble techniques is that multiple diverse models, when combined intelligently, can outperform a single model by balancing errors and improving overall predictive performance.\n"
      ],
      "metadata": {
        "id": "ZwAgYxDHPsfq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J6_HcKwuOYVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **12. What is a Random Forest Classifier?**\n",
        "\n",
        "**What is a Random Forest Classifier?**\n",
        "\n",
        "A **Random Forest Classifier** is an **ensemble learning algorithm** used for classification tasks that builds multiple decision trees and combines their predictions to produce a more accurate and robust final result.\n",
        "\n",
        "It is based on the idea of **‚Äúwisdom of the crowd‚Äù**, where a group of weak learners (decision trees) together form a strong learner.\n",
        "\n",
        "---\n",
        "\n",
        "### Working Principle of Random Forest Classifier\n",
        "\n",
        "**Step 1: Bootstrap Sampling (Bagging)**  \n",
        "From the original training dataset, multiple random subsets are created using **sampling with replacement**. Each subset may contain duplicate samples and omit some original data points.\n",
        "\n",
        "**Step 2: Building Multiple Decision Trees**  \n",
        "A decision tree is trained on each bootstrapped dataset.  \n",
        "Unlike a normal decision tree, Random Forest introduces **feature randomness**:  \n",
        "- At each split, only a **random subset of features** is considered instead of all features.  \n",
        "This increases diversity among trees.\n",
        "\n",
        "**Step 3: Independent Learning**  \n",
        "Each tree learns independently and makes its own classification decision based on the data it has seen.\n",
        "\n",
        "**Step 4: Aggregation Using Majority Voting**  \n",
        "When a new data point is given:\n",
        "- All trees predict a class label\n",
        "- The final output is the class that receives the **maximum number of votes** (majority voting)\n",
        "\n",
        "---\n",
        "\n",
        "### Key Characteristics\n",
        "\n",
        "- Uses **multiple decision trees**\n",
        "- Combines **Bagging + Feature Randomness**\n",
        "- Reduces **overfitting** compared to a single decision tree\n",
        "- Works well with **high-dimensional data**\n",
        "- Handles **nonlinear relationships** effectively\n",
        "\n",
        "---\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- High accuracy and strong generalization\n",
        "- Robust to noise and outliers\n",
        "- Reduces variance\n",
        "- Can estimate **feature importance**\n",
        "- Works well without heavy data preprocessing\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- Computationally expensive for large datasets\n",
        "- Less interpretable than a single decision tree\n",
        "- Requires more memory\n",
        "\n",
        "---\n",
        "\n",
        "### Example (Conceptual)\n",
        "\n",
        "If 100 trees are used and:\n",
        "- 60 trees predict **Class A**\n",
        "- 40 trees predict **Class B**\n",
        "\n",
        "Final Prediction = **Class A**\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "A Random Forest Classifier is a powerful ensemble algorithm that improves classification performance by combining multiple decision trees trained on different data samples and feature subsets, producing accurate, stable, and reliable predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "pDYHcnBAPuM1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8LEMSg5OYR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **13. What are the main types of ensemble techniques?**\n",
        "\n",
        "### üìö Types:\n",
        "1. Bagging\n",
        "2. Boosting\n",
        "3. Stacking\n",
        "4. Voting\n",
        "\n",
        "\n",
        "\n",
        "Ensemble techniques are methods that combine multiple machine learning models to improve accuracy, robustness, and generalization. The main types of ensemble techniques are explained below in detail:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Bagging (Bootstrap Aggregating)\n",
        "\n",
        "Bagging works by generating multiple training datasets from the original dataset using **bootstrap sampling** (random sampling with replacement). Each dataset is used to train an independent base model, usually a high-variance model like a decision tree. The final prediction is obtained by combining the predictions of all models using **majority voting** for classification or **averaging** for regression. Bagging primarily helps in **reducing variance** and preventing overfitting.\n",
        "\n",
        "**Example:** Random Forest (uses bagging with feature randomness)\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Boosting\n",
        "\n",
        "Boosting is a **sequential ensemble technique** where models are trained one after another. Each new model focuses more on the data points that were misclassified by previous models by assigning them higher weights. Predictions from all models are combined using **weighted voting**. Boosting helps in **reducing bias and variance** and improves performance on difficult samples.\n",
        "\n",
        "**Examples:** AdaBoost, Gradient Boosting, XGBoost, LightGBM\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Stacking (Stacked Generalization)\n",
        "\n",
        "Stacking combines predictions from multiple different base models by using a **meta-learner**. First, several base models (level-0 models) are trained on the dataset. Their predictions are then used as input features to train a second-level model (level-1 or meta-model), which learns how to best combine these predictions to make the final output.\n",
        "\n",
        "**Example:** Combining Decision Tree, SVM, and Logistic Regression using Logistic Regression as meta-model\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Voting Ensemble\n",
        "\n",
        "Voting ensembles combine predictions from multiple models trained on the same dataset. In **hard voting**, the class with the majority of votes is selected. In **soft voting**, predicted probabilities from all models are averaged, and the class with the highest probability is chosen. Voting ensembles improve stability and overall performance.\n",
        "\n",
        "**Example:** VotingClassifier in scikit-learn\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "The main ensemble techniques are **Bagging, Boosting, Stacking, and Voting**. Each technique improves model performance in a different way by reducing bias, variance, or both, leading to more accurate and robust predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "oiSK1exHPv3X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K5iTY00QOYOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **14. What is ensemble learning in machine learning?**\n",
        "\n",
        "### üîç Definition:\n",
        "Combining multiple models to improve prediction performance.\n",
        "\n",
        "### üß† Goal:\n",
        "Reduce bias, variance, or both\n",
        "\n",
        "Ensemble learning is a machine learning approach in which **multiple models (called base learners or weak learners) are trained and combined to solve the same problem**, with the goal of achieving better performance than any single model alone.\n",
        "\n",
        "Instead of relying on one model, ensemble learning uses the **collective intelligence of several models**, based on the idea that different models learn different patterns and make different errors.\n",
        "\n",
        "---\n",
        "\n",
        "### How Ensemble Learning Works\n",
        "\n",
        "1. **Train Multiple Models**  \n",
        "   Several models are trained on the same dataset or on different subsets of the dataset. These models can be the same algorithm (homogeneous ensemble) or different algorithms (heterogeneous ensemble).\n",
        "\n",
        "2. **Create Diversity Among Models**  \n",
        "   Diversity is introduced by:\n",
        "   - Using different training samples (Bagging)\n",
        "   - Using different feature subsets (Random Forest)\n",
        "   - Changing sample weights (Boosting)\n",
        "   - Using different algorithms (Stacking)\n",
        "\n",
        "3. **Individual Predictions**  \n",
        "   Each model independently makes a prediction for new data.\n",
        "\n",
        "4. **Combine Predictions**  \n",
        "   Predictions are aggregated using:\n",
        "   - **Majority voting** (classification)\n",
        "   - **Averaging** (regression)\n",
        "   - **Weighted voting** (boosting)\n",
        "\n",
        "---\n",
        "\n",
        "### Why Ensemble Learning Works\n",
        "\n",
        "- Reduces **variance**\n",
        "- Reduces **bias**\n",
        "- Minimizes **overfitting**\n",
        "- Improves **generalization**\n",
        "- Produces more stable and accurate predictions\n",
        "\n",
        "---\n",
        "\n",
        "### Types of Ensemble Learning\n",
        "\n",
        "- **Bagging** ‚Äì Reduces variance\n",
        "- **Boosting** ‚Äì Reduces bias and variance\n",
        "- **Stacking** ‚Äì Learns optimal combination of models\n",
        "- **Voting** ‚Äì Simple aggregation of models\n",
        "\n",
        "---\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Higher accuracy\n",
        "- Better robustness to noise\n",
        "- Improved performance on complex datasets\n",
        "\n",
        "---\n",
        "\n",
        "### Limitations\n",
        "\n",
        "- Increased computational cost\n",
        "- Reduced interpretability\n",
        "- More complex model design\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ensemble learning is a powerful machine learning technique that improves prediction accuracy and reliability by combining multiple diverse models, making it one of the most effective approaches in modern machine learning.\n"
      ],
      "metadata": {
        "id": "8mYHrL5yPxcE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "doRXXEKrOYLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **15. When should we avoid using ensemble methods?**\n",
        "\n",
        "### ‚ùå Avoid when:\n",
        "- Dataset is very small\n",
        "- Interpretability is critical\n",
        "- Computational resources are limited\n",
        "\n",
        "\n",
        "Ensemble methods are powerful, but they are not always the best choice. They should be avoided or used cautiously in the following situations:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Limited Computational Resources  \n",
        "Ensemble methods require training and maintaining multiple models, which increases **training time, memory usage, and prediction latency**. When hardware resources or time are limited (e.g., real-time systems or edge devices), a single simpler model is often more practical.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Need for High Interpretability  \n",
        "Ensemble models are often considered **black-box models**. If the application requires clear explanations and transparency (such as in healthcare, finance, or legal domains), simpler and more interpretable models like linear regression or single decision trees are preferable.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Small or Simple Datasets  \n",
        "For small datasets or problems with simple patterns, ensemble methods may **overcomplicate the solution** without significant performance gains. A single well-tuned model can perform just as well or better.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Real-Time or Low-Latency Applications  \n",
        "Since ensemble methods combine predictions from many models, they may be **too slow for real-time predictions**, especially when the ensemble size is large.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Minimal Performance Improvement  \n",
        "If adding more models does not significantly improve performance, ensemble methods may lead to **diminishing returns**, increasing complexity without meaningful benefits.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Difficulty in Model Maintenance  \n",
        "Ensembles are harder to maintain, update, and deploy compared to single models. In environments requiring frequent updates, simpler models may be easier to manage.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "Ensemble methods should be avoided when simplicity, interpretability, speed, or resource efficiency are more important than marginal gains in accuracy. Choosing the right model depends on the problem requirements and constraints.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAc2QQNPPybs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_k619PunOYIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9tZZoo7P888"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **16. How does Bagging help in reducing overfitting?**\n",
        "\n",
        "\n",
        "Bagging (Bootstrap Aggregating) helps in reducing overfitting by **lowering the variance of machine learning models**, especially high-variance models like decision trees. Overfitting occurs when a model learns noise and details from the training data instead of the general pattern.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Explanation\n",
        "\n",
        "**1. Bootstrap Sampling**  \n",
        "Bagging creates multiple different training datasets by randomly sampling the original dataset **with replacement**. Each bootstrap sample contains duplicate observations and leaves out some original data points. This ensures that each model is trained on a slightly different dataset.\n",
        "\n",
        "**2. Training Multiple Independent Models**  \n",
        "A separate base learner is trained on each bootstrap dataset. Since the training data differs, each model learns different patterns and overfits to different noise in the data.\n",
        "\n",
        "**3. Diversity in Predictions**  \n",
        "Because each model is different, their prediction errors are also different and often uncorrelated. This diversity is essential for reducing overfitting.\n",
        "\n",
        "**4. Aggregation of Predictions**  \n",
        "The predictions from all models are combined:\n",
        "- **Majority voting** for classification\n",
        "- **Averaging** for regression  \n",
        "This aggregation smooths out individual model errors.\n",
        "\n",
        "**5. Variance Reduction**  \n",
        "Overfitting is mainly caused by high variance. By averaging multiple models, Bagging cancels out random fluctuations and noise learned by individual models, resulting in a more stable and generalized model.\n",
        "\n",
        "---\n",
        "\n",
        "### Why Bagging Is Effective\n",
        "\n",
        "- Reduces variance without increasing bias significantly\n",
        "- Prevents models from memorizing training data\n",
        "- Improves performance on unseen data\n",
        "- Works best with unstable learners (e.g., decision trees)\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Bagging reduces overfitting by training multiple models on different bootstrapped datasets and combining their predictions. This process averages out noise and reduces variance, leading to better generalization and improved performance on new data.\n"
      ],
      "metadata": {
        "id": "Bsst5B4zP_8E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E2gh6O-CP9wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **17. Why is Random Forest better than a single Decision Tree?**\n",
        "\n",
        "Random Forest is considered better than a single Decision Tree because it **improves accuracy, reduces overfitting, and provides more robust and reliable predictions** by combining the power of multiple trees instead of relying on just one.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Reduction in Overfitting  \n",
        "A single Decision Tree tends to overfit the training data, especially when it grows deep. Random Forest builds many trees on different bootstrap samples and averages their predictions. This process **reduces variance** and prevents the model from memorizing noise in the data.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Higher Prediction Accuracy  \n",
        "Random Forest combines predictions from multiple trees using **majority voting**. Even if some trees make incorrect predictions, others correct them, leading to **higher overall accuracy** than a single Decision Tree.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Feature Randomness  \n",
        "In Random Forest, only a **random subset of features** is considered at each split. This prevents dominant features from controlling the entire model and increases diversity among trees, improving generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Robustness to Noise and Outliers  \n",
        "A single Decision Tree is highly sensitive to noisy data and outliers. Random Forest dilutes the effect of noise because not all trees see the same noisy samples, making the model more stable.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Better Generalization  \n",
        "By averaging multiple diverse trees, Random Forest learns more generalized patterns that perform well on unseen data, unlike a single Decision Tree that may capture dataset-specific patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Built-in Feature Importance  \n",
        "Random Forest provides built-in methods to measure **feature importance**, helping in feature selection and model interpretation, which is not as reliable in a single Decision Tree.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Handles High-Dimensional Data Well  \n",
        "Random Forest performs well even when the number of features is large and captures complex, non-linear relationships more effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion  \n",
        "Random Forest outperforms a single Decision Tree by reducing overfitting, improving accuracy, increasing robustness, and offering better generalization. It transforms many weak, overfitting trees into a strong and reliable ensemble model.\n"
      ],
      "metadata": {
        "id": "BhZOddAqQBi3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42Dc1v0BP9tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **18. What is the role of bootstrap sampling in Bagging?**\n",
        "\n",
        "\n",
        "\n",
        "Bootstrap sampling plays a **core role** in Bagging (Bootstrap Aggregating) by creating **diverse training datasets** from the original dataset, which helps improve model stability and reduce overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### Step-by-Step Role of Bootstrap Sampling\n",
        "\n",
        "**1. Creating Multiple Datasets**  \n",
        "Bootstrap sampling generates several new training datasets by randomly sampling data points **with replacement** from the original dataset. Each bootstrap dataset has the same size as the original dataset but contains duplicate observations and leaves out some data points.\n",
        "\n",
        "**2. Introducing Diversity Among Models**  \n",
        "Because each model is trained on a different bootstrap sample, the models learn different patterns. This diversity ensures that the models make **uncorrelated errors**, which is essential for effective ensemble learning.\n",
        "\n",
        "**3. Enabling Independent Model Training**  \n",
        "Each bootstrap dataset is used to train a separate base learner independently. This independence allows models to capture different aspects of the data.\n",
        "\n",
        "**4. Reducing Overfitting**  \n",
        "High-variance models like decision trees tend to overfit. Bootstrap sampling helps prevent this by ensuring no single model sees the entire dataset, reducing the chance of memorizing noise.\n",
        "\n",
        "**5. Supporting Variance Reduction**  \n",
        "When predictions from multiple models are aggregated (through majority voting or averaging), random errors cancel out, leading to reduced variance and improved generalization.\n",
        "\n",
        "---\n",
        "\n",
        "### Key Benefits of Bootstrap Sampling in Bagging\n",
        "\n",
        "- Creates multiple unique training sets\n",
        "- Increases model diversity\n",
        "- Reduces variance\n",
        "- Improves robustness\n",
        "- Enhances generalization performance\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Bootstrap sampling is the foundation of Bagging. By generating diverse training datasets and enabling independent model training, it helps reduce overfitting, stabilize predictions, and improve overall model performance.\n"
      ],
      "metadata": {
        "id": "Mrymr9JzQDxo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JgV-uzQTP9qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **19. What are some real-world applications of ensemble techniques?**\n",
        "\n",
        "\n",
        "Ensemble techniques are widely used in real-world applications because they provide **high accuracy, robustness, and reliability** by combining multiple models. Some important real-world applications are explained below:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Finance and Banking  \n",
        "Ensemble models are used for **credit scoring, loan default prediction, fraud detection, and risk assessment**. By combining multiple models, banks can reduce false positives and improve decision reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Healthcare and Medical Diagnosis  \n",
        "In healthcare, ensemble techniques help in **disease diagnosis, medical image analysis, and patient risk prediction**. Combining multiple models improves diagnostic accuracy and reduces misclassification risks.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. E-commerce and Recommendation Systems  \n",
        "Ensembles are used in **product recommendation systems**, customer behavior prediction, and demand forecasting. They improve personalization by aggregating insights from different models.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Computer Vision  \n",
        "Tasks like **face recognition, object detection, and image classification** use ensemble models to handle variations in lighting, angles, and noise, improving recognition accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Natural Language Processing (NLP)  \n",
        "Ensemble techniques are applied in **spam detection, sentiment analysis, machine translation, and text classification**. They help capture different linguistic patterns and reduce misinterpretation.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Autonomous Systems  \n",
        "Self-driving cars and robotics use ensemble models for **object detection, path planning, and decision-making**, ensuring safer and more reliable predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Weather Forecasting  \n",
        "Ensemble learning improves **weather and climate prediction** by combining multiple forecasting models, reducing uncertainty and increasing reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Cybersecurity  \n",
        "Ensemble techniques are used in **intrusion detection systems and malware classification**, where combining models helps detect threats more accurately.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Ensemble techniques are applied across industries such as finance, healthcare, e-commerce, vision systems, NLP, and cybersecurity because they enhance accuracy, robustness, and decision-making reliability in complex real-world problems.\n"
      ],
      "metadata": {
        "id": "GkCU4bZVQFPy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_UNvQ6NP9ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **20. What is the difference between Bagging and Boosting?**\n",
        "\n",
        "**What is the difference between Bagging and Boosting?**\n",
        "\n",
        "Bagging (Bootstrap Aggregating) and Boosting are both ensemble learning techniques, but they differ significantly in how models are trained, how errors are handled, and what problem they mainly address.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Training Style\n",
        "\n",
        "**Bagging:**  \n",
        "- Models are trained **independently and in parallel**.  \n",
        "- Each model does not depend on the performance of other models.\n",
        "\n",
        "**Boosting:**  \n",
        "- Models are trained **sequentially**.  \n",
        "- Each new model depends on the errors made by the previous models.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Data Sampling Method\n",
        "\n",
        "**Bagging:**  \n",
        "- Uses **bootstrap sampling** (sampling with replacement).  \n",
        "- Each model is trained on a randomly sampled dataset from the original data.\n",
        "\n",
        "**Boosting:**  \n",
        "- Uses the **entire dataset**, but changes the **weights of data points**.  \n",
        "- Misclassified samples are given higher importance in subsequent models.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Focus on Errors\n",
        "\n",
        "**Bagging:**  \n",
        "- Treats all data points equally.  \n",
        "- Does not focus specifically on difficult or misclassified samples.\n",
        "\n",
        "**Boosting:**  \n",
        "- Focuses strongly on **misclassified or hard-to-predict samples**.  \n",
        "- Each new model tries to correct the mistakes of the previous ones.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Bias and Variance Reduction\n",
        "\n",
        "**Bagging:**  \n",
        "- Mainly reduces **variance**.  \n",
        "- Best suited for **high-variance models** (e.g., decision trees).\n",
        "\n",
        "**Boosting:**  \n",
        "- Reduces both **bias and variance**.  \n",
        "- Helps weak learners become strong learners.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Model Combination Method\n",
        "\n",
        "**Bagging:**  \n",
        "- Combines predictions using **majority voting** (classification) or **averaging** (regression).\n",
        "\n",
        "**Boosting:**  \n",
        "- Combines predictions using **weighted voting**, where better-performing models have more influence.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Sensitivity to Noise\n",
        "\n",
        "**Bagging:**  \n",
        "- More robust to noisy data and outliers.  \n",
        "- Noise impact is reduced through averaging.\n",
        "\n",
        "**Boosting:**  \n",
        "- More sensitive to noise and outliers.  \n",
        "- Can overfit noisy data by repeatedly focusing on difficult samples.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Computational Aspect\n",
        "\n",
        "**Bagging:**  \n",
        "- Easier to parallelize.  \n",
        "- Faster training due to independent models.\n",
        "\n",
        "**Boosting:**  \n",
        "- Harder to parallelize due to sequential dependency.  \n",
        "- Generally slower to train.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Common Examples\n",
        "\n",
        "**Bagging:**  \n",
        "- Random Forest  \n",
        "- Bagging Classifier\n",
        "\n",
        "**Boosting:**  \n",
        "- AdaBoost  \n",
        "- Gradient Boosting  \n",
        "- XGBoost  \n",
        "- LightGBM\n",
        "\n",
        "---\n",
        "\n",
        "### Summary Table\n",
        "\n",
        "| Aspect | Bagging | Boosting |\n",
        "|------|--------|---------|\n",
        "| Training | Parallel | Sequential |\n",
        "| Sampling | Bootstrap sampling | Weighted samples |\n",
        "| Error Focus | No special focus | Focus on misclassified samples |\n",
        "| Main Benefit | Variance reduction | Bias + variance reduction |\n",
        "| Noise Sensitivity | Low | High |\n",
        "| Speed | Faster | Slower |\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Bagging improves model stability by reducing variance through independent models, while Boosting builds a strong learner by sequentially correcting errors. The choice between them depends on the problem, dataset size, noise level, and computational constraints.\n",
        "\n"
      ],
      "metadata": {
        "id": "f2lBWa_YQHIm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GDPdpXErP9eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##practical"
      ],
      "metadata": {
        "id": "h2IMXHqtQKN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy**\n"
      ],
      "metadata": {
        "id": "1nrnIp_-RP8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Create a sample (synthetic) dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define base estimator\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Step 4: Initialize Bagging Classifier (FIXED PARAMETER)\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    estimator=dt,          # ‚úÖ correct parameter\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train model\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Predict\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Step 7: Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1uSHWP-QNbS",
        "outputId": "12133398-3201-4485-f297-487053233379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9466666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "obQojry_SGcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)**\n"
      ],
      "metadata": {
        "id": "ZJytVa9nSG_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Create a sample (synthetic) regression dataset\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    noise=15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define base estimator (Decision Tree Regressor)\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Step 4: Initialize Bagging Regressor (latest sklearn syntax)\n",
        "bagging_regressor = BaggingRegressor(\n",
        "    estimator=dt_regressor,   # ‚úÖ correct parameter\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train the Bagging Regressor\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9liMujDRQNX7",
        "outputId": "1e6c13e4-d631-47aa-82b0-5c4e827f22ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 442.6320453924955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Blhp8faAQNVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores**\n"
      ],
      "metadata": {
        "id": "fFV8ERzhTNmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Initialize Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Extract feature importance scores\n",
        "importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Step 6: Print feature importance scores\n",
        "print(\"Feature Importance Scores:\\n\")\n",
        "for feature, importance in zip(feature_names, importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HbaWxDQQNSu",
        "outputId": "8fbec2fe-78aa-4cb4-e28c-161817d5c319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "\n",
            "mean radius: 0.0323\n",
            "mean texture: 0.0111\n",
            "mean perimeter: 0.0601\n",
            "mean area: 0.0538\n",
            "mean smoothness: 0.0062\n",
            "mean compactness: 0.0092\n",
            "mean concavity: 0.0806\n",
            "mean concave points: 0.1419\n",
            "mean symmetry: 0.0033\n",
            "mean fractal dimension: 0.0031\n",
            "radius error: 0.0164\n",
            "texture error: 0.0032\n",
            "perimeter error: 0.0118\n",
            "area error: 0.0295\n",
            "smoothness error: 0.0059\n",
            "compactness error: 0.0046\n",
            "concavity error: 0.0058\n",
            "concave points error: 0.0034\n",
            "symmetry error: 0.0040\n",
            "fractal dimension error: 0.0071\n",
            "worst radius: 0.0780\n",
            "worst texture: 0.0188\n",
            "worst perimeter: 0.0743\n",
            "worst area: 0.1182\n",
            "worst smoothness: 0.0118\n",
            "worst compactness: 0.0175\n",
            "worst concavity: 0.0411\n",
            "worst concave points: 0.1271\n",
            "worst symmetry: 0.0129\n",
            "worst fractal dimension: 0.0069\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zk6g9Cd1QNP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **24. Train a Random Forest Regressor and compare its performance with a single Decision Tree**\n"
      ],
      "metadata": {
        "id": "ZvCjpRgJTuPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load the Ames Housing regression dataset from openml\n",
        "housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Handle potential missing values and non-numeric data\n",
        "# For simplicity, we'll drop non-numeric columns and fill NaNs for now\n",
        "X = X.select_dtypes(include=['number']).fillna(X.select_dtypes(include=['number']).mean())\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Initialize models\n",
        "decision_tree = DecisionTreeRegressor(random_state=42)\n",
        "random_forest = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train both models\n",
        "decision_tree.fit(X_train, y_train)\n",
        "random_forest.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "dt_predictions = decision_tree.predict(X_test)\n",
        "rf_predictions = random_forest.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate performance using Mean Squared Error (MSE)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"Decision Tree Regressor MSE:\", dt_mse)\n",
        "print(\"Random Forest Regressor MSE:\", rf_mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2UgFkzeTvSu",
        "outputId": "b5c15f6e-b7df-4684-870b-d22868afcb1e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Regressor MSE: 1272802122.525114\n",
            "Random Forest Regressor MSE: 693005906.9860406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6_93_DhxTwS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier**\n"
      ],
      "metadata": {
        "id": "eFmTRVJ-UvLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Initialize Random Forest Classifier with OOB enabled\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,      # Required for OOB\n",
        "    oob_score=True,      # Enable OOB score\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train the model\n",
        "rf_classifier.fit(X, y)\n",
        "\n",
        "# Step 4: Print the OOB score\n",
        "print(\"Out-of-Bag (OOB) Score:\", rf_classifier.oob_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8Tm0yquTwPg",
        "outputId": "2f3a327e-0900-4b18-d33a-141efb8b1eb2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Out-of-Bag (OOB) Score: 0.961335676625659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6tjDHXQFTwBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **26. Train a Bagging Classifier using SVM as a base estimator and print accuracy**\n"
      ],
      "metadata": {
        "id": "Qzuw8t2LU_48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Create a sample (synthetic) classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=5,\n",
        "    n_redundant=2,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define SVM as the base estimator\n",
        "svm_model = SVC(\n",
        "    kernel='rbf',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Initialize Bagging Classifier with SVM (latest sklearn syntax)\n",
        "bagging_svm = BaggingClassifier(\n",
        "    estimator=svm_model,   # ‚úÖ correct parameter\n",
        "    n_estimators=10,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train the Bagging Classifier\n",
        "bagging_svm.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "y_pred = bagging_svm.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Bagging Classifier (SVM) Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qhbjx4Y0Tv9q",
        "outputId": "c8ab11e7-9fd3-4db0-c1ad-e403b51259b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier (SVM) Accuracy: 0.9133333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VTIHxKQRTv6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **27. Train a Random Forest Classifier with different numbers of trees and compare accuracy**\n"
      ],
      "metadata": {
        "id": "ukDmSkXFVL5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Random Forest models with different numbers of trees\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    rf_classifier = RandomForestClassifier(\n",
        "        n_estimators=n,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"Number of Trees: {n}, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23KOR3VqTv3g",
        "outputId": "07fca863-2473-4c54-d33b-8089fa005294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Trees: 10, Accuracy: 0.9649122807017544\n",
            "Number of Trees: 50, Accuracy: 0.9707602339181286\n",
            "Number of Trees: 100, Accuracy: 0.9707602339181286\n",
            "Number of Trees: 200, Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgMS54CgVM58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score**\n"
      ],
      "metadata": {
        "id": "Qcgh5C5WVVIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Bagging Classifier using Logistic Regression and print AUC score\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# 1. Create a sample binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define Logistic Regression as the base estimator\n",
        "base_estimator = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# 4. Create the Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=base_estimator,\n",
        "    n_estimators=50,\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 5. Train the Bagging Classifier\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "# 6. Predict probabilities on the test set\n",
        "y_pred_proba = bagging_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# 7. Calculate and print AUC score\n",
        "auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "print(\"AUC Score:\", auc_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bD-_7lRJQZbL",
        "outputId": "3fc01489-6c74-4d1b-a923-4b114730bced"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC Score: 0.9136160714285714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CC4VBrKEVNiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **29. Train a Random Forest Regressor and analyze feature importance scores**\n"
      ],
      "metadata": {
        "id": "3L8Y1lTGV48H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Step 1: Load the California Housing dataset\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "feature_names = fetch_california_housing().feature_names\n",
        "\n",
        "# Step 2: Initialize Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train the model\n",
        "rf_regressor.fit(X, y)\n",
        "\n",
        "# Step 4: Extract feature importance scores\n",
        "importances = rf_regressor.feature_importances_\n",
        "\n",
        "# Step 5: Print feature importance scores\n",
        "print(\"Feature Importance Scores:\\n\")\n",
        "for feature, importance in zip(feature_names, importances):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3utVBBqVNfU",
        "outputId": "4d3ed25f-2446-4be3-fed3-59e945930d94"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance Scores:\n",
            "\n",
            "MedInc: 0.5200\n",
            "HouseAge: 0.0530\n",
            "AveRooms: 0.0445\n",
            "AveBedrms: 0.0293\n",
            "Population: 0.0312\n",
            "AveOccup: 0.1364\n",
            "Latitude: 0.0929\n",
            "Longitude: 0.0927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8iHDUzOEVNZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **30. Train an ensemble model using both Bagging and Random Forest and compare accuracy**\n"
      ],
      "metadata": {
        "id": "yU_fyyNRWBH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Initialize Bagging Classifier\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Initialize Random Forest Classifier\n",
        "random_forest_clf = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 5: Train both models\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "random_forest_clf.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions\n",
        "bagging_pred = bagging_clf.predict(X_test)\n",
        "rf_pred = random_forest_clf.predict(X_test)\n",
        "\n",
        "# Step 7: Evaluate and compare accuracy\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n",
        "print(\"Random Forest Classifier Accuracy:\", rf_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDBE1kAZVNSx",
        "outputId": "2dddeaa3-bc40-47a1-ca72-38aeb684f0a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9590643274853801\n",
            "Random Forest Classifier Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HtzFs-d2VNP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV**\n"
      ],
      "metadata": {
        "id": "Hlo4BxFZYgGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Step 4: Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 5, 10],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Step 5: Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Perform grid search\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Get the best model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Step 8: Evaluate the best model on test data\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Test Accuracy with Best Model:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljcWPz_zVNB8",
        "outputId": "765d1bf8-56d0-44ff-99e5-76d39cbec9e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
            "Test Accuracy with Best Model: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6begzIfuVM-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **32. Train a Bagging Regressor with different numbers of base estimators and compare performance**\n"
      ],
      "metadata": {
        "id": "fsS_rycTYowR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Create a sample (synthetic) regression dataset\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    noise=15,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define base estimator\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Step 4: Train Bagging Regressor with different numbers of estimators\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    bagging_regressor = BaggingRegressor(\n",
        "        estimator=dt_regressor,   # latest sklearn syntax\n",
        "        n_estimators=n,\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Evaluate using MSE\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"Number of Estimators: {n}, MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VS4p4k25TvyA",
        "outputId": "e0c82138-7e16-4c9f-e45d-224bfed7d86d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Estimators: 10, MSE: 497.90899211531496\n",
            "Number of Estimators: 50, MSE: 455.0745611324985\n",
            "Number of Estimators: 100, MSE: 442.6320453924955\n",
            "Number of Estimators: 200, MSE: 432.5751194054189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pmy0_2v-X8sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DygMel2kX-Do"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **33. Train a Random Forest Classifier and analyze misclassified samples**\n"
      ],
      "metadata": {
        "id": "Kg7qf8VSYub2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names = data.target_names\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate overall accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Step 7: Identify misclassified samples\n",
        "misclassified_indices = np.where(y_test != y_pred)[0]\n",
        "\n",
        "print(\"Number of Misclassified Samples:\", len(misclassified_indices))\n",
        "\n",
        "# Step 8: Display details of a few misclassified samples\n",
        "print(\"\\nSample Misclassified Observations (first 5):\\n\")\n",
        "for idx in misclassified_indices[:5]:\n",
        "    print(f\"Index: {idx}\")\n",
        "    print(\"True Label:\", target_names[y_test[idx]])\n",
        "    print(\"Predicted Label:\", target_names[y_pred[idx]])\n",
        "    print(\"Feature Values:\", X_test[idx])\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rj6DUqiX-AY",
        "outputId": "f9e876de-f2b0-4ed4-9f71-6856d2f717d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9707602339181286\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       0.98      0.94      0.96        63\n",
            "      benign       0.96      0.99      0.98       108\n",
            "\n",
            "    accuracy                           0.97       171\n",
            "   macro avg       0.97      0.96      0.97       171\n",
            "weighted avg       0.97      0.97      0.97       171\n",
            "\n",
            "Number of Misclassified Samples: 5\n",
            "\n",
            "Sample Misclassified Observations (first 5):\n",
            "\n",
            "Index: 8\n",
            "True Label: benign\n",
            "Predicted Label: malignant\n",
            "Feature Values: [1.334e+01 1.586e+01 8.649e+01 5.200e+02 1.078e-01 1.535e-01 1.169e-01\n",
            " 6.987e-02 1.942e-01 6.902e-02 2.860e-01 1.016e+00 1.535e+00 1.296e+01\n",
            " 6.794e-03 3.575e-02 3.980e-02 1.383e-02 2.134e-02 4.603e-03 1.553e+01\n",
            " 2.319e+01 9.666e+01 6.149e+02 1.536e-01 4.791e-01 4.858e-01 1.708e-01\n",
            " 3.527e-01 1.016e-01]\n",
            "--------------------------------------------------\n",
            "Index: 20\n",
            "True Label: malignant\n",
            "Predicted Label: benign\n",
            "Feature Values: [1.380e+01 1.579e+01 9.043e+01 5.841e+02 1.007e-01 1.280e-01 7.789e-02\n",
            " 5.069e-02 1.662e-01 6.566e-02 2.787e-01 6.205e-01 1.957e+00 2.335e+01\n",
            " 4.717e-03 2.065e-02 1.759e-02 9.206e-03 1.220e-02 3.130e-03 1.657e+01\n",
            " 2.086e+01 1.103e+02 8.124e+02 1.411e-01 3.542e-01 2.779e-01 1.383e-01\n",
            " 2.589e-01 1.030e-01]\n",
            "--------------------------------------------------\n",
            "Index: 77\n",
            "True Label: malignant\n",
            "Predicted Label: benign\n",
            "Feature Values: [1.396e+01 1.705e+01 9.143e+01 6.024e+02 1.096e-01 1.279e-01 9.789e-02\n",
            " 5.246e-02 1.908e-01 6.130e-02 4.250e-01 8.098e-01 2.563e+00 3.574e+01\n",
            " 6.351e-03 2.679e-02 3.119e-02 1.342e-02 2.062e-02 2.695e-03 1.639e+01\n",
            " 2.207e+01 1.081e+02 8.260e+02 1.512e-01 3.262e-01 3.209e-01 1.374e-01\n",
            " 3.068e-01 7.957e-02]\n",
            "--------------------------------------------------\n",
            "Index: 82\n",
            "True Label: malignant\n",
            "Predicted Label: benign\n",
            "Feature Values: [1.448e+01 2.146e+01 9.425e+01 6.482e+02 9.444e-02 9.947e-02 1.204e-01\n",
            " 4.938e-02 2.075e-01 5.636e-02 4.204e-01 2.220e+00 3.301e+00 3.887e+01\n",
            " 9.369e-03 2.983e-02 5.371e-02 1.761e-02 2.418e-02 3.249e-03 1.621e+01\n",
            " 2.925e+01 1.084e+02 8.089e+02 1.306e-01 1.976e-01 3.349e-01 1.225e-01\n",
            " 3.020e-01 6.846e-02]\n",
            "--------------------------------------------------\n",
            "Index: 164\n",
            "True Label: malignant\n",
            "Predicted Label: benign\n",
            "Feature Values: [1.513e+01 2.981e+01 9.671e+01 7.195e+02 8.320e-02 4.605e-02 4.686e-02\n",
            " 2.739e-02 1.852e-01 5.294e-02 4.681e-01 1.627e+00 3.043e+00 4.538e+01\n",
            " 6.831e-03 1.427e-02 2.489e-02 9.087e-03 3.151e-02 1.750e-03 1.726e+01\n",
            " 3.691e+01 1.101e+02 9.314e+02 1.148e-01 9.866e-02 1.547e-01 6.575e-02\n",
            " 3.233e-01 6.165e-02]\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XmRVJaqRX95u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier**\n"
      ],
      "metadata": {
        "id": "LD0gBOJVYzdd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train a single Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Train a Bagging Classifier with Decision Trees\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions\n",
        "dt_pred = dt_classifier.predict(X_test)\n",
        "bagging_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate accuracy\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "# Step 7: Print comparison results\n",
        "print(\"Decision Tree Accuracy:\", dt_accuracy)\n",
        "print(\"Bagging Classifier Accuracy:\", bagging_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDhjxA-DX92M",
        "outputId": "1ab9b679-327b-4fd3-c867-764baeadda36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.9415204678362573\n",
            "Bagging Classifier Accuracy: 0.9590643274853801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LiEq2x2X9yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3kNRxwEgX9uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **35. Train a Random Forest Classifier and visualize the confusion matrix**\n"
      ],
      "metadata": {
        "id": "keQrPTekY3Mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "class_names = data.target_names\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Compute accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# Step 6: Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 7: Visualize confusion matrix\n",
        "disp = ConfusionMatrixDisplay(\n",
        "    confusion_matrix=cm,\n",
        "    display_labels=class_names\n",
        ")\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - Random Forest Classifier\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "KwAqnrNhX9q8",
        "outputId": "b178b63f-8ba7-4321-9d32-b0e41bffc202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9707602339181286\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eMelDOb3X9nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy**\n"
      ],
      "metadata": {
        "id": "PcupYN76Y6s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Stacking Classifier using Decision Tree, SVM, and Logistic Regression\n",
        "# and compare accuracy with individual models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Create a sample classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define base models\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "svm = SVC(probability=True, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# 4. Train individual models\n",
        "dt.fit(X_train, y_train)\n",
        "svm.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate individual model accuracy\n",
        "dt_acc = accuracy_score(y_test, dt.predict(X_test))\n",
        "svm_acc = accuracy_score(y_test, svm.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "\n",
        "# 6. Define Stacking Classifier\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('decision_tree', dt),\n",
        "        ('svm', svm),\n",
        "        ('logistic_regression', lr)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 7. Train Stacking Classifier\n",
        "stack_clf.fit(X_train, y_train)\n",
        "\n",
        "# 8. Evaluate Stacking Classifier accuracy\n",
        "stack_acc = accuracy_score(y_test, stack_clf.predict(X_test))\n",
        "\n",
        "# 9. Print accuracy comparison\n",
        "print(\"Decision Tree Accuracy:\", dt_acc)\n",
        "print(\"SVM Accuracy:\", svm_acc)\n",
        "print(\"Logistic Regression Accuracy:\", lr_acc)\n",
        "print(\"Stacking Classifier Accuracy:\", stack_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzcvpIcbQ2uC",
        "outputId": "29c16f4b-909d-4382-80cc-691cd0c03bfd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 0.8166666666666667\n",
            "SVM Accuracy: 0.95\n",
            "Logistic Regression Accuracy: 0.8366666666666667\n",
            "Stacking Classifier Accuracy: 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JeMwZRPEX9gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1k4x9PmX9c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **37. Train a Random Forest Classifier and print the top 5 most important features**\n"
      ],
      "metadata": {
        "id": "lOHLlHoLY_DE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Get feature importance scores\n",
        "importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Step 5: Sort features by importance (descending)\n",
        "indices = np.argsort(importances)[::-1]\n",
        "\n",
        "# Step 6: Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\\n\")\n",
        "for i in range(5):\n",
        "    idx = indices[i]\n",
        "    print(f\"{i+1}. {feature_names[idx]} - Importance: {importances[idx]:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B72D6WsMX9ZK",
        "outputId": "9891e798-c596-47d9-8079-ac735525a253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "\n",
            "1. mean concave points - Importance: 0.1419\n",
            "2. worst concave points - Importance: 0.1271\n",
            "3. worst area - Importance: 0.1182\n",
            "4. mean concavity - Importance: 0.0806\n",
            "5. worst radius - Importance: 0.0780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SeySSMAfX9Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score**\n"
      ],
      "metadata": {
        "id": "3RCrV_Q-ZCgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Bagging Classifier with Decision Trees\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate using Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "\n",
        "# Step 6: Detailed classification report\n",
        "print(\"\\nClassification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kayx42NLX9Sc",
        "outputId": "e4705027-3bd1-4733-d389-9e5d664b27ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.963302752293578\n",
            "Recall: 0.9722222222222222\n",
            "F1-score: 0.967741935483871\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.94        63\n",
            "           1       0.96      0.97      0.97       108\n",
            "\n",
            "    accuracy                           0.96       171\n",
            "   macro avg       0.96      0.95      0.96       171\n",
            "weighted avg       0.96      0.96      0.96       171\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nx7eE5O1X9Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KfRh3CtuX8oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy**\n"
      ],
      "metadata": {
        "id": "bedRgrpdZGNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define different max_depth values to test\n",
        "max_depth_values = [None, 2, 4, 6, 8, 10]\n",
        "\n",
        "# Step 4: Train Random Forest models with different max_depth values\n",
        "for depth in max_depth_values:\n",
        "    rf_classifier = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=depth,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"max_depth = {depth}, Accuracy = {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jlm8hcOX8lX",
        "outputId": "d2695d92-ddba-45d9-ecac-5ca644062a4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_depth = None, Accuracy = 0.9707602339181286\n",
            "max_depth = 2, Accuracy = 0.9532163742690059\n",
            "max_depth = 4, Accuracy = 0.9707602339181286\n",
            "max_depth = 6, Accuracy = 0.9649122807017544\n",
            "max_depth = 8, Accuracy = 0.9707602339181286\n",
            "max_depth = 10, Accuracy = 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tVDH3YZmX8iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJoEWSRcX8eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance**\n"
      ],
      "metadata": {
        "id": "USfxioZiZKLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Create a sample (synthetic) regression dataset\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    noise=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define base estimators\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "\n",
        "# Step 4: Train Bagging Regressor with Decision Tree\n",
        "bagging_dt = BaggingRegressor(\n",
        "    estimator=dt_regressor,\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_dt.fit(X_train, y_train)\n",
        "dt_predictions = bagging_dt.predict(X_test)\n",
        "dt_mse = mean_squared_error(y_test, dt_predictions)\n",
        "\n",
        "# Step 5: Train Bagging Regressor with KNN\n",
        "bagging_knn = BaggingRegressor(\n",
        "    estimator=knn_regressor,\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_knn.fit(X_train, y_train)\n",
        "knn_predictions = bagging_knn.predict(X_test)\n",
        "knn_mse = mean_squared_error(y_test, knn_predictions)\n",
        "\n",
        "# Step 6: Print performance comparison\n",
        "print(\"Bagging Regressor with Decision Tree - MSE:\", dt_mse)\n",
        "print(\"Bagging Regressor with KNeighbors - MSE:\", knn_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXfFd7NFX8bq",
        "outputId": "48743867-78a9-46e4-9518-1e5a802c172c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor with Decision Tree - MSE: 626.199194944707\n",
            "Bagging Regressor with KNeighbors - MSE: 1279.419867364259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nm8ftb6XX8Yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S4mWQ_5aX8VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **41. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score**\n"
      ],
      "metadata": {
        "id": "lpew9COXZPp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities for the positive class\n",
        "y_prob = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mg8eZ58tX8Ru",
        "outputId": "442032b6-4f98-424a-a6be-78be6da8377a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9968400940623163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nkzRelLrX8Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **42. Train a Bagging Classifier and evaluate its performance using Cross-Validation**\n"
      ],
      "metadata": {
        "id": "Z0SVHXjdZWTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Initialize Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(random_state=42),\n",
        "    n_estimators=100,\n",
        "    bootstrap=True,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Perform Cross-Validation\n",
        "cv_scores = cross_val_score(\n",
        "    bagging_classifier,\n",
        "    X,\n",
        "    y,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "\n",
        "# Step 4: Print Cross-Validation results\n",
        "print(\"Cross-Validation Accuracy Scores:\", cv_scores)\n",
        "print(\"Mean CV Accuracy:\", cv_scores.mean())\n",
        "print(\"Standard Deviation:\", cv_scores.std())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90k8rIGTX8K3",
        "outputId": "290e6e37-c150-4149-c7c2-111929bc9963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracy Scores: [0.89473684 0.93859649 0.99122807 0.96491228 1.        ]\n",
            "Mean CV Accuracy: 0.9578947368421054\n",
            "Standard Deviation: 0.03819568606504778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oLiAiGejZSwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **43. Train a Random Forest Classifier and plot the Precision‚ÄìRecall curve**\n"
      ],
      "metadata": {
        "id": "SUp-2mBsZaKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict probabilities for the positive class\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Step 5: Compute Precision-Recall values\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Step 6: Compute Average Precision (AP) score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "print(\"Average Precision Score:\", avg_precision)\n",
        "\n",
        "# Step 7: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, linewidth=2, label=f'AP = {avg_precision:.3f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision‚ÄìRecall Curve (Random Forest Classifier)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "jluJMrNxZSsm",
        "outputId": "d4e0d548-6213-4d7b-f755-0eddad450e91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Precision Score: 0.9981193532697757\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NWK_3GK9ZSpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xr6gscXjZSlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy**\n"
      ],
      "metadata": {
        "id": "3SXGODHOZfrp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Stacking Classifier with Random Forest and Logistic Regression\n",
        "# and compare accuracy with individual models\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Create a sample classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=20,\n",
        "    n_informative=10,\n",
        "    n_redundant=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define base models\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# 4. Train individual models\n",
        "rf.fit(X_train, y_train)\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate individual model accuracy\n",
        "rf_acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "lr_acc = accuracy_score(y_test, lr.predict(X_test))\n",
        "\n",
        "# 6. Define Stacking Classifier\n",
        "stack_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('random_forest', rf),\n",
        "        ('logistic_regression', lr)\n",
        "    ],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 7. Train Stacking Classifier\n",
        "stack_clf.fit(X_train, y_train)\n",
        "\n",
        "# 8. Evaluate Stacking Classifier accuracy\n",
        "stack_acc = accuracy_score(y_test, stack_clf.predict(X_test))\n",
        "\n",
        "# 9. Print accuracy comparison\n",
        "print(\"Random Forest Accuracy:\", rf_acc)\n",
        "print(\"Logistic Regression Accuracy:\", lr_acc)\n",
        "print(\"Stacking Classifier Accuracy:\", stack_acc)\n"
      ],
      "metadata": {
        "id": "gXsavs_8ZkVY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ee9d1a-30ff-4f61-980b-756e83f6a51b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9166666666666666\n",
            "Logistic Regression Accuracy: 0.8366666666666667\n",
            "Stacking Classifier Accuracy: 0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance**\n"
      ],
      "metadata": {
        "id": "wWutUXImZkwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Create a sample (synthetic) regression dataset\n",
        "X, y = make_regression(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_informative=6,\n",
        "    noise=20,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define base estimator\n",
        "base_regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Step 4: Define different bootstrap sample fractions\n",
        "bootstrap_levels = [0.5, 0.7, 1.0]   # 50%, 70%, 100% of training data\n",
        "\n",
        "# Step 5: Train and evaluate Bagging Regressor for each level\n",
        "for fraction in bootstrap_levels:\n",
        "    bagging_regressor = BaggingRegressor(\n",
        "        estimator=base_regressor,\n",
        "        n_estimators=100,\n",
        "        max_samples=fraction,   # level of bootstrap sampling\n",
        "        bootstrap=True,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "    # Predict\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "    # Evaluate using Mean Squared Error\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    print(f\"Bootstrap Sample Fraction: {fraction}, MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4_GuSHeZkQX",
        "outputId": "9eb2e016-403e-442f-efe5-371d1c3119cc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bootstrap Sample Fraction: 0.5, MSE: 638.7730442275351\n",
            "Bootstrap Sample Fraction: 0.7, MSE: 607.950780611301\n",
            "Bootstrap Sample Fraction: 1.0, MSE: 626.199194944707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m_6LVnR0ZkMp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}