{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "29141643",
      "metadata": {
        "id": "29141643"
      },
      "source": [
        "\n",
        "### **Question 1: What is Simple Linear Regression**\n",
        "\n",
        "**Simple Linear Regression** is a statistical method used to model the relationship between **one independent variable (X)** and **one dependent variable (Y)** by fitting a straight line through the observed data points.\n",
        "\n",
        "The primary goal of simple linear regression is to **predict the value of the dependent variable** based on the independent variable and to **understand how changes in X affect Y**.\n",
        "\n",
        "\n",
        "### **Mathematical Equation**\n",
        "\\[\n",
        "Y = mX + c\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **Y** → Dependent (response) variable  \n",
        "- **X** → Independent (predictor) variable  \n",
        "- **m** → Slope of the regression line  \n",
        "- **c** → Intercept of the regression line  \n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation**\n",
        "- The **slope (m)** represents the change in Y for a one-unit increase in X.\n",
        "  - If **m > 0**, Y increases as X increases.\n",
        "  - If **m < 0**, Y decreases as X increases.\n",
        "- The **intercept (c)** indicates the value of Y when X equals zero.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "If **X** represents years of experience and **Y** represents salary, simple linear regression helps:\n",
        "- Estimate how much salary increases with each additional year of experience\n",
        "- Predict salary for a given number of years of experience\n",
        "\n",
        "---\n",
        "\n",
        "### **Applications of Simple Linear Regression**\n",
        "- Prediction and forecasting  \n",
        "- Identifying linear relationships between variables  \n",
        "- Trend analysis  \n",
        "- Decision-making based on data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hX698mVwgW_e"
      },
      "id": "hX698mVwgW_e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 2: What are the key assumptions of Simple Linear Regression**\n",
        "\n",
        "Simple Linear Regression relies on several fundamental assumptions to ensure that the model produces valid, reliable, and unbiased results. If these assumptions are violated, the accuracy and interpretability of the regression model may be affected.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Assumptions**\n",
        "\n",
        "1. **Linearity**\n",
        "   - There must be a **linear relationship** between the independent variable (X) and the dependent variable (Y).\n",
        "   - This means changes in X should result in proportional changes in Y.\n",
        "\n",
        "2. **Independence**\n",
        "   - The observations should be **independent of each other**.\n",
        "   - The value of one observation should not influence another (no autocorrelation).\n",
        "\n",
        "3. **Homoscedasticity**\n",
        "   - The variance of the residuals (errors) should be **constant across all values of X**.\n",
        "   - If variance changes, it leads to heteroscedasticity, which can reduce model reliability.\n",
        "\n",
        "4. **Normality of Errors**\n",
        "   - The residuals should be **normally distributed**, especially important for hypothesis testing and confidence intervals.\n",
        "   - This does not require X or Y to be normally distributed, only the errors.\n",
        "\n",
        "5. **No Multicollinearity (Trivially Satisfied)**\n",
        "   - Since simple linear regression has only one predictor, multicollinearity is **not an issue**.\n",
        "   - This assumption becomes important in multiple linear regression.\n",
        "\n",
        "6. **Zero Mean of Errors**\n",
        "   - The expected value of the error term should be **zero**, meaning the model does not systematically overestimate or underestimate Y.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why These Assumptions Matter**\n",
        "- They ensure accurate coefficient estimates\n",
        "- They make hypothesis testing valid\n",
        "- They improve prediction reliability\n",
        "- They help in correct interpretation of the regression model\n",
        "\n",
        "---\n",
        "\n",
        "If any of these assumptions are violated, corrective measures such as data transformation or alternative models may be required.\n"
      ],
      "metadata": {
        "id": "VaqOxFFrgYX8"
      },
      "id": "VaqOxFFrgYX8"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B52qttoghQeP"
      },
      "id": "B52qttoghQeP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 3: What does the coefficient m represent in the equation Y = mX + c**\n",
        "\n",
        "In the simple linear regression equation **Y = mX + c**, the coefficient **m** represents the **slope of the regression line**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Meaning of the Coefficient m**\n",
        "\n",
        "- The coefficient **m** indicates the **rate of change of the dependent variable (Y)** with respect to a **one-unit change in the independent variable (X)**.\n",
        "- It quantifies **how much Y is expected to increase or decrease when X increases by one unit**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation of m**\n",
        "\n",
        "- **m > 0 (Positive slope):**\n",
        "  - As X increases, Y also increases.\n",
        "  - Indicates a **positive linear relationship** between X and Y.\n",
        "\n",
        "- **m < 0 (Negative slope):**\n",
        "  - As X increases, Y decreases.\n",
        "  - Indicates a **negative linear relationship** between X and Y.\n",
        "\n",
        "- **m = 0:**\n",
        "  - No linear relationship between X and Y.\n",
        "  - Y remains constant regardless of X.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "If **m = 5**, it means that:\n",
        "- For every **1-unit increase in X**, the value of **Y increases by 5 units**, assuming all other factors remain constant.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why the Coefficient m is Important**\n",
        "- It determines the **direction** of the relationship\n",
        "- It measures the **strength of the relationship**\n",
        "- It plays a crucial role in **prediction and interpretation** of the regression model\n"
      ],
      "metadata": {
        "id": "EILu9a_BgWQp"
      },
      "id": "EILu9a_BgWQp"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkLOazrWhf5X"
      },
      "id": "vkLOazrWhf5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 4: What does the intercept c represent in the equation Y = mX + c**\n",
        "\n",
        "In the simple linear regression equation **Y = mX + c**, the intercept **c** represents the **value of the dependent variable (Y) when the independent variable (X) is equal to zero**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Meaning of the Intercept c**\n",
        "\n",
        "- The intercept provides a **baseline or starting point** for the regression line.\n",
        "- It shows where the regression line **crosses the Y-axis**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation of c**\n",
        "\n",
        "- When **X = 0**, the predicted value of **Y = c**.\n",
        "- The intercept helps in understanding the **initial level** of the dependent variable before any influence of the independent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "If a regression model predicting salary based on years of experience is:\n",
        "\n",
        "\\[\n",
        "Y = 5000X + 20000\n",
        "\\]\n",
        "\n",
        "- Here, **c = 20000**\n",
        "- This means the predicted salary is **20,000** when years of experience (**X**) is zero.\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Considerations**\n",
        "- The intercept is **meaningful only if X = 0 is within the range of the data**.\n",
        "- If X = 0 is not realistic (e.g., negative age or experience), the intercept may not have a practical interpretation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why the Intercept is Important**\n",
        "- Provides context for the regression equation\n",
        "- Helps in forming predictions\n",
        "- Assists in understanding the relationship between variables\n"
      ],
      "metadata": {
        "id": "f55NIuzqhgbI"
      },
      "id": "f55NIuzqhgbI"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nUqk2LVChuy7"
      },
      "id": "nUqk2LVChuy7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 5: How do we calculate the slope m in Simple Linear Regression**\n",
        "\n",
        "In Simple Linear Regression, the slope **m** is calculated using the **least squares method**, which minimizes the sum of the squared differences between the actual values and the predicted values.\n",
        "\n",
        "---\n",
        "\n",
        "### **Formula for Calculating the Slope (m)**\n",
        "\n",
        "\\[\n",
        "m = \\frac{n\\sum XY - (\\sum X)(\\sum Y)}{n\\sum X^2 - (\\sum X)^2}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **n** → Number of observations  \n",
        "- **X** → Independent variable values  \n",
        "- **Y** → Dependent variable values  \n",
        "- **∑XY** → Sum of the product of X and Y  \n",
        "- **∑X²** → Sum of squared X values  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### **Explanation**\n",
        "- The numerator measures how X and Y vary together.\n",
        "- The denominator measures how X varies from its mean.\n",
        "- This ratio ensures the best-fitting line by minimizing prediction errors.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Formula is Important**\n",
        "- Provides the most accurate linear fit\n",
        "- Reduces prediction errors\n",
        "- Forms the foundation of linear regression modeling\n"
      ],
      "metadata": {
        "id": "LqBUgkvvhvRc"
      },
      "id": "LqBUgkvvhvRc"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8djFm6syhwpP"
      },
      "id": "8djFm6syhwpP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 6: What is the purpose of the least squares method in Simple Linear Regression**\n",
        "\n",
        "The **least squares method** is used in Simple Linear Regression to determine the **best-fitting straight line** that represents the relationship between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "---\n",
        "\n",
        "### **Purpose of the Least Squares Method**\n",
        "\n",
        "- To **minimize the total error** between the observed values and the predicted values.\n",
        "- To ensure the regression line is as close as possible to all data points.\n",
        "\n",
        "---\n",
        "\n",
        "### **How It Works**\n",
        "\n",
        "- For each data point, the **residual** is calculated as:  \n",
        "  \\[\n",
        "  \\text{Residual} = \\text{Actual Y} - \\text{Predicted Y}\n",
        "  \\]\n",
        "- The least squares method minimizes the **sum of squared residuals**:\n",
        "  \\[\n",
        "  \\sum (Y - \\hat{Y})^2\n",
        "  \\]\n",
        "- Squaring ensures that:\n",
        "  - Positive and negative errors do not cancel each other out\n",
        "  - Larger errors are penalized more heavily\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Squared Errors Are Used**\n",
        "- Avoids cancellation of errors\n",
        "- Makes the optimization mathematically tractable\n",
        "- Emphasizes larger deviations\n",
        "\n",
        "---\n",
        "\n",
        "### **Importance of the Least Squares Method**\n",
        "- Produces **optimal estimates** of slope and intercept\n",
        "- Provides a **unique solution** for the regression line\n",
        "- Forms the foundation of most regression techniques\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "The least squares method ensures that the regression model fits the data in the **most accurate and reliable way possible**, making it essential for effective prediction and interpretation.\n"
      ],
      "metadata": {
        "id": "J4NnAHg8hw7v"
      },
      "id": "J4NnAHg8hw7v"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r997DQcOhxmt"
      },
      "id": "r997DQcOhxmt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 7: How is the coefficient of determination (R²) interpreted in Simple Linear Regression**\n",
        "\n",
        "The **coefficient of determination (R²)** is a statistical measure that explains **how well the independent variable (X) explains the variability in the dependent variable (Y)** in a Simple Linear Regression model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Meaning of R²**\n",
        "\n",
        "- R² represents the **proportion of the total variance in Y that is explained by X**.\n",
        "- Its value ranges from **0 to 1** (or 0% to 100%).\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation of R² Values**\n",
        "\n",
        "- **R² = 0**\n",
        "  - The model explains **none** of the variability in Y.\n",
        "  - X has no explanatory power.\n",
        "\n",
        "- **R² = 1**\n",
        "  - The model explains **all** the variability in Y.\n",
        "  - Perfect linear relationship.\n",
        "\n",
        "- **0 < R² < 1**\n",
        "  - Indicates partial explanation of variability.\n",
        "  - For example, **R² = 0.75** means **75% of the variation in Y is explained by X**, while 25% is due to other factors or random error.\n",
        "\n",
        "---\n",
        "\n",
        "### **How R² Is Calculated**\n",
        "\n",
        "\\[\n",
        "R^2 = 1 - \\frac{\\text{Sum of Squared Residuals (SSR)}}{\\text{Total Sum of Squares (SST)}}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **SSR** → Unexplained variation\n",
        "- **SST** → Total variation in Y\n",
        "\n",
        "---\n",
        "\n",
        "### **Why R² Is Important**\n",
        "- Measures **goodness of fit** of the regression model\n",
        "- Helps compare models\n",
        "- Indicates how useful X is in predicting Y\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations of R²**\n",
        "- Does not indicate causation\n",
        "- A high R² does not always mean the model is appropriate\n",
        "- Should be used along with other metrics and diagnostic checks\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "R² provides a clear and intuitive way to understand how well a simple linear regression model explains the relationship between variables, but it should not be used in isolation.\n"
      ],
      "metadata": {
        "id": "Vqc7nYU3hx3B"
      },
      "id": "Vqc7nYU3hx3B"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eaB--On1jAXg"
      },
      "id": "eaB--On1jAXg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 8: What is Multiple Linear Regression**\n",
        "\n",
        "**Multiple Linear Regression (MLR)** is a statistical technique used to model the relationship between **one dependent variable (Y)** and **two or more independent variables (X₁, X₂, X₃, …)**.\n",
        "\n",
        "It extends simple linear regression by allowing **multiple predictors** to explain and predict the outcome variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Equation**\n",
        "\n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + \\cdots + b_nX_n\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **Y** → Dependent (response) variable  \n",
        "- **X₁, X₂, …, Xₙ** → Independent (predictor) variables  \n",
        "- **b₀** → Intercept  \n",
        "- **b₁, b₂, …, bₙ** → Regression coefficients representing the effect of each predictor  \n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation**\n",
        "- Each coefficient (**bᵢ**) represents the **change in Y for a one-unit change in Xᵢ**, **holding all other variables constant**.\n",
        "- The model evaluates the **combined effect of multiple variables** on the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "Predicting **house price (Y)** based on:\n",
        "- Size of the house (X₁)\n",
        "- Number of bedrooms (X₂)\n",
        "- Location score (X₃)\n",
        "\n",
        "Multiple linear regression helps determine how each factor contributes to the final price.\n",
        "\n",
        "---\n",
        "\n",
        "### **Applications of Multiple Linear Regression**\n",
        "- Economic and financial forecasting  \n",
        "- Business and marketing analytics  \n",
        "- Risk assessment  \n",
        "- Social science and medical research  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Multiple Linear Regression provides a more realistic and powerful modeling approach when the outcome variable is influenced by **multiple factors**, making it widely used in real-world data analysis.\n"
      ],
      "metadata": {
        "id": "pCkRKHIShREJ"
      },
      "id": "pCkRKHIShREJ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wOj738XWjXag"
      },
      "id": "wOj738XWjXag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 9: What is the main difference between Simple and Multiple Linear Regression**\n",
        "\n",
        "The main difference between **Simple Linear Regression** and **Multiple Linear Regression** lies in the **number of independent variables used to predict the dependent variable**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Simple Linear Regression**\n",
        "- Uses **one independent variable** to predict the dependent variable.\n",
        "- Examines the relationship between **two variables only**.\n",
        "- Equation:\n",
        "\\[\n",
        "Y = mX + c\n",
        "\\]\n",
        "\n",
        "**Example:**  \n",
        "Predicting salary based only on years of experience.\n",
        "\n",
        "---\n",
        "\n",
        "### **Multiple Linear Regression**\n",
        "- Uses **two or more independent variables** to predict the dependent variable.\n",
        "- Examines the combined effect of multiple predictors on Y.\n",
        "- Equation:\n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + \\cdots + b_nX_n\n",
        "\\]\n",
        "\n",
        "**Example:**  \n",
        "Predicting salary based on experience, education level, and job role.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences Summary**\n",
        "\n",
        "| Aspect | Simple Linear Regression | Multiple Linear Regression |\n",
        "|------|-------------------------|----------------------------|\n",
        "| Number of predictors | One | Two or more |\n",
        "| Complexity | Simple | More complex |\n",
        "| Real-world applicability | Limited | More realistic |\n",
        "| Interpretation | Straightforward | Requires careful analysis |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Simple Linear Regression is suitable for analyzing relationships involving a single factor, while Multiple Linear Regression is more powerful and practical when the outcome depends on **multiple influencing variables**.\n"
      ],
      "metadata": {
        "id": "roYhnMyBjYo4"
      },
      "id": "roYhnMyBjYo4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_wYcpybKjZeT"
      },
      "id": "_wYcpybKjZeT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 10: What are the key assumptions of Multiple Linear Regression**\n",
        "\n",
        "Multiple Linear Regression (MLR) is based on several important assumptions that must be satisfied to ensure the validity, accuracy, and reliability of the model’s results.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Assumptions**\n",
        "\n",
        "1. **Linearity**\n",
        "   - There must be a **linear relationship** between the dependent variable and each independent variable.\n",
        "   - The effect of each predictor on Y should be linear when other variables are held constant.\n",
        "\n",
        "2. **Independence of Errors**\n",
        "   - The residuals (errors) should be **independent of each other**.\n",
        "   - Violation of this assumption may lead to autocorrelation.\n",
        "\n",
        "3. **Homoscedasticity**\n",
        "   - The variance of residuals should be **constant across all levels of the independent variables**.\n",
        "   - Non-constant variance leads to heteroscedasticity, which can distort inference.\n",
        "\n",
        "4. **Normality of Errors**\n",
        "   - The residuals should be **normally distributed**, especially important for hypothesis testing and confidence intervals.\n",
        "\n",
        "5. **No Multicollinearity**\n",
        "   - Independent variables should not be **highly correlated with each other**.\n",
        "   - High multicollinearity makes coefficient estimates unstable and difficult to interpret.\n",
        "\n",
        "6. **No Perfect Multicollinearity**\n",
        "   - No independent variable should be a perfect linear combination of other predictors.\n",
        "\n",
        "7. **Correct Model Specification**\n",
        "   - The model should include all relevant variables and exclude irrelevant ones to avoid bias.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why These Assumptions Matter**\n",
        "- Ensure unbiased and efficient coefficient estimates  \n",
        "- Improve prediction accuracy  \n",
        "- Make statistical tests valid  \n",
        "- Enhance interpretability of the model  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Meeting these assumptions allows Multiple Linear Regression to provide meaningful insights into the relationship between multiple predictors and the dependent variable. When assumptions are violated, corrective measures such as transformation or alternative models should be considered.\n"
      ],
      "metadata": {
        "id": "w1lSd1wRjZ6J"
      },
      "id": "w1lSd1wRjZ6J"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SUpRILOzjbH3"
      },
      "id": "SUpRILOzjbH3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 11: What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model**\n",
        "\n",
        "**Heteroscedasticity** refers to a situation in a regression model where the **variance of the error terms (residuals) is not constant** across all levels of the independent variables.\n",
        "\n",
        "In Multiple Linear Regression, this violates one of the key assumptions of the model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Understanding Heteroscedasticity**\n",
        "\n",
        "- When residual variance **changes with the level of predictors**, the data is heteroscedastic.\n",
        "- It is often observed as a **funnel-shaped or uneven spread** of residuals in a residual plot.\n",
        "\n",
        "---\n",
        "\n",
        "### **Effects on Multiple Linear Regression Results**\n",
        "\n",
        "1. **Unreliable Standard Errors**\n",
        "   - Coefficient estimates remain unbiased, but their **standard errors become incorrect**.\n",
        "\n",
        "2. **Invalid Hypothesis Testing**\n",
        "   - t-tests and F-tests may give **misleading p-values**, leading to wrong conclusions.\n",
        "\n",
        "3. **Reduced Efficiency**\n",
        "   - The model is no longer the best linear unbiased estimator (BLUE).\n",
        "\n",
        "4. **Misleading Confidence Intervals**\n",
        "   - Confidence intervals may be too wide or too narrow.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Causes**\n",
        "- Presence of outliers  \n",
        "- Skewed distributions  \n",
        "- Incorrect model specification  \n",
        "- Large differences in scale among variables  \n",
        "\n",
        "---\n",
        "\n",
        "### **How to Address Heteroscedasticity**\n",
        "- Transform the dependent variable (e.g., log, square root)\n",
        "- Use weighted least squares\n",
        "- Apply robust standard errors\n",
        "- Improve model specification\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Heteroscedasticity does not bias coefficient estimates, but it significantly affects the **reliability of statistical inference**. Detecting and correcting it is crucial for building a trustworthy Multiple Linear Regression model.\n"
      ],
      "metadata": {
        "id": "lSBAWtjYjbgA"
      },
      "id": "lSBAWtjYjbgA"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z3g_T6eZjdBG"
      },
      "id": "z3g_T6eZjdBG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 12: How can you improve a Multiple Linear Regression model with high multicollinearity**\n",
        "\n",
        "**Multicollinearity** occurs in Multiple Linear Regression when two or more independent variables are **highly correlated with each other**, making it difficult to accurately estimate the individual effect of each predictor.\n",
        "\n",
        "High multicollinearity does not reduce the model’s predictive power but **negatively affects the reliability and interpretability of regression coefficients**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Problems Caused by High Multicollinearity**\n",
        "- Unstable and highly sensitive coefficient estimates  \n",
        "- Large standard errors for regression coefficients  \n",
        "- Difficulty in determining the importance of individual predictors  \n",
        "- Insignificant p-values despite a high R²  \n",
        "\n",
        "---\n",
        "\n",
        "### **Techniques to Improve the Model**\n",
        "\n",
        "1. **Remove Highly Correlated Variables**\n",
        "   - Identify correlated predictors using a **correlation matrix** or **Variance Inflation Factor (VIF)**.\n",
        "   - Remove one of the correlated variables if it does not add unique information.\n",
        "\n",
        "2. **Combine Variables**\n",
        "   - Create a **composite variable** (e.g., average or sum) when predictors measure similar concepts.\n",
        "\n",
        "3. **Apply Dimensionality Reduction**\n",
        "   - Use techniques like **Principal Component Analysis (PCA)** to reduce correlated features into independent components.\n",
        "\n",
        "4. **Regularization Techniques**\n",
        "   - Apply **Ridge Regression** or **Lasso Regression**, which penalize large coefficients and reduce multicollinearity effects.\n",
        "\n",
        "5. **Center or Standardize Variables**\n",
        "   - Subtract the mean or scale variables to reduce correlation caused by interaction terms or polynomial features.\n",
        "\n",
        "6. **Increase Sample Size**\n",
        "   - A larger dataset can sometimes reduce the severity of multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Detect Multicollinearity**\n",
        "- High correlation coefficients (close to ±1)\n",
        "- High **VIF values** (VIF > 10 is a common warning sign)\n",
        "- Large changes in coefficients when variables are added or removed\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Improving a Multiple Linear Regression model with high multicollinearity involves **diagnosing correlated predictors** and applying strategies such as feature selection, transformation, or regularization to ensure **stable, interpretable, and reliable results**.\n"
      ],
      "metadata": {
        "id": "SeL5qvvAjdbq"
      },
      "id": "SeL5qvvAjdbq"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "81o1wyxCjeI-"
      },
      "id": "81o1wyxCjeI-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 13: What are some common techniques for transforming categorical variables for use in regression models**\n",
        "\n",
        "Categorical variables must be **converted into numerical form** before they can be used in regression models, as regression algorithms operate on numerical data. Several techniques are commonly used to transform categorical variables depending on the nature of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Techniques**\n",
        "\n",
        "1. **One-Hot Encoding (Dummy Variables)**\n",
        "   - Creates a new binary (0/1) column for each category.\n",
        "   - Avoids implying any ordinal relationship.\n",
        "   - One category is usually dropped to avoid the **dummy variable trap**.\n",
        "\n",
        "   **Example:**  \n",
        "   Color → Red, Blue, Green  \n",
        "   → Red (1/0), Blue (1/0), Green (1/0)\n",
        "\n",
        "2. **Label Encoding**\n",
        "   - Assigns a unique numerical value to each category.\n",
        "   - Suitable when categories have **no meaningful order only for tree-based models**, not ideal for linear regression.\n",
        "\n",
        "   **Example:**  \n",
        "   Low = 0, Medium = 1, High = 2\n",
        "\n",
        "3. **Ordinal Encoding**\n",
        "   - Used when categories have a **natural order**.\n",
        "   - Preserves ranking information.\n",
        "\n",
        "   **Example:**  \n",
        "   Education level: High School < Bachelor < Master < PhD\n",
        "\n",
        "4. **Binary Encoding**\n",
        "   - Converts categories into binary digits.\n",
        "   - Reduces dimensionality compared to one-hot encoding.\n",
        "   - Useful for high-cardinality categorical variables.\n",
        "\n",
        "5. **Target Encoding (Mean Encoding)**\n",
        "   - Replaces categories with the **mean of the target variable** for that category.\n",
        "   - Must be used carefully to avoid data leakage.\n",
        "\n",
        "6. **Frequency Encoding**\n",
        "   - Replaces each category with its **frequency or proportion** in the dataset.\n",
        "   - Useful when category occurrence carries information.\n",
        "\n",
        "---\n",
        "\n",
        "### **Choosing the Right Technique**\n",
        "- **Nominal variables** → One-hot encoding  \n",
        "- **Ordinal variables** → Ordinal encoding  \n",
        "- **High-cardinality variables** → Binary or target encoding  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Transforming categorical variables appropriately is essential for building effective regression models. The choice of technique depends on the **type of categorical data**, the **model used**, and the **interpretability requirements**.\n"
      ],
      "metadata": {
        "id": "3-sAHflUjejo"
      },
      "id": "3-sAHflUjejo"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VLk8F7spkoiE"
      },
      "id": "VLk8F7spkoiE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 14: What is the role of interaction terms in Multiple Linear Regression**\n",
        "\n",
        "**Interaction terms** in Multiple Linear Regression are used to capture situations where the **effect of one independent variable on the dependent variable depends on the value of another independent variable**.\n",
        "\n",
        "They help model **combined or joint effects** that cannot be explained by individual predictors alone.\n",
        "\n",
        "---\n",
        "\n",
        "### **What Is an Interaction Term**\n",
        "\n",
        "- An interaction term is created by **multiplying two or more independent variables**.\n",
        "- It allows the relationship between X and Y to **change depending on another variable**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Mathematical Representation**\n",
        "\n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + b_3(X_1 \\times X_2) + \\varepsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **X₁ × X₂** is the interaction term\n",
        "- **b₃** measures how the effect of X₁ on Y changes with X₂\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Interaction Terms Are Important**\n",
        "\n",
        "1. **Capture Real-World Complexity**\n",
        "   - Real-world relationships are often not purely additive.\n",
        "   - Interaction terms reflect more realistic scenarios.\n",
        "\n",
        "2. **Improve Model Accuracy**\n",
        "   - Including interactions can significantly improve model fit when joint effects exist.\n",
        "\n",
        "3. **Enhance Interpretability**\n",
        "   - Helps explain conditional relationships between variables.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "Suppose we model salary based on:\n",
        "- Years of experience (X₁)\n",
        "- Education level (X₂)\n",
        "\n",
        "An interaction term (X₁ × X₂) allows the model to capture the idea that:\n",
        "- The impact of experience on salary may differ by education level.\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Considerations**\n",
        "- Interaction terms increase model complexity.\n",
        "- Variables should often be **centered or standardized** before creating interactions to reduce multicollinearity.\n",
        "- Only include interaction terms when theoretically justified.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Interaction terms play a crucial role in Multiple Linear Regression by modeling how predictors **work together**, enabling more accurate and meaningful interpretations of complex relationships.\n"
      ],
      "metadata": {
        "id": "vp2CE1AkkpCy"
      },
      "id": "vp2CE1AkkpCy"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rcgml95WlAYQ"
      },
      "id": "Rcgml95WlAYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 15: How can the interpretation of the intercept differ between Simple and Multiple Linear Regression**\n",
        "\n",
        "The **intercept** in a regression model represents the expected value of the dependent variable when **all independent variables are equal to zero**. However, its interpretation differs between **Simple Linear Regression** and **Multiple Linear Regression** due to the number of predictors involved.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation in Simple Linear Regression**\n",
        "\n",
        "- In Simple Linear Regression, there is **only one independent variable**.\n",
        "- The intercept represents the value of **Y when X = 0**.\n",
        "- It is often easier to interpret and may have practical meaning if X = 0 is within the data range.\n",
        "\n",
        "**Example:**  \n",
        "If salary is predicted from years of experience, the intercept represents the predicted salary when experience is zero.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation in Multiple Linear Regression**\n",
        "\n",
        "- In Multiple Linear Regression, the intercept represents the value of **Y when all independent variables are zero simultaneously**.\n",
        "- This situation may be **theoretical or unrealistic**, making interpretation more abstract.\n",
        "\n",
        "**Example:**  \n",
        "In a model predicting house price using size, number of rooms, and location score, the intercept represents the price when all predictors are zero, which may not be meaningful in practice.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**\n",
        "\n",
        "| Aspect | Simple Linear Regression | Multiple Linear Regression |\n",
        "|------|-------------------------|----------------------------|\n",
        "| Number of predictors | One | Two or more |\n",
        "| Interpretability | Often straightforward | Often theoretical |\n",
        "| Practical meaning | More likely | Less likely |\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Difference Matters**\n",
        "- Helps avoid misinterpretation of model outputs\n",
        "- Encourages focus on **slopes and interactions** in multiple regression\n",
        "- Improves model understanding and communication\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "While the intercept in Simple Linear Regression often has a clear interpretation, in Multiple Linear Regression it may serve primarily as a **mathematical reference point** rather than a meaningful real-world value.\n"
      ],
      "metadata": {
        "id": "4jlXUteHlBBI"
      },
      "id": "4jlXUteHlBBI"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tngggTrDlBmK"
      },
      "id": "tngggTrDlBmK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 16: What is the significance of the slope in regression analysis, and how does it affect predictions**\n",
        "\n",
        "The **slope** in a regression model represents the **rate at which the dependent variable (Y) changes with respect to a one-unit change in an independent variable (X)**, while keeping other variables constant (in multiple regression).\n",
        "\n",
        "---\n",
        "\n",
        "### **Significance of the Slope**\n",
        "\n",
        "- The slope quantifies the **strength and direction** of the relationship between variables.\n",
        "- It indicates whether the relationship is:\n",
        "  - **Positive** (slope > 0): Y increases as X increases  \n",
        "  - **Negative** (slope < 0): Y decreases as X increases  \n",
        "  - **Zero** (slope = 0): No linear relationship  \n",
        "\n",
        "---\n",
        "\n",
        "### **Statistical Significance**\n",
        "- Statistical tests (t-test) are used to determine whether the slope is **significantly different from zero**.\n",
        "- A statistically significant slope suggests that the predictor has a **real effect** on the outcome variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **How the Slope Affects Predictions**\n",
        "\n",
        "- The slope directly determines how predictions change when X changes.\n",
        "- A **larger absolute value** of the slope means:\n",
        "  - Stronger influence of X on Y\n",
        "  - Greater sensitivity of predictions to changes in X\n",
        "- In Multiple Linear Regression, each slope affects predictions **independently**, assuming other variables remain constant.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "If the slope of experience in a salary model is **3,000**, it means:\n",
        "- For every additional year of experience, the predicted salary increases by **3,000 units**, assuming other factors stay the same.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why the Slope Is Important**\n",
        "- Core component of model interpretation  \n",
        "- Drives prediction behavior  \n",
        "- Helps identify influential variables  \n",
        "- Supports data-driven decision-making  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "The slope is a crucial element in regression analysis as it explains **how and how much** the dependent variable responds to changes in predictors, directly shaping both interpretation and prediction accuracy.\n"
      ],
      "metadata": {
        "id": "oS1tYh4WlCJB"
      },
      "id": "oS1tYh4WlCJB"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pCxSUrXulCoO"
      },
      "id": "pCxSUrXulCoO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 17: How does the intercept in a regression model provide context for the relationship between variables**\n",
        "\n",
        "The **intercept** in a regression model provides a **reference point** that helps place the relationship between the independent and dependent variables into context.\n",
        "\n",
        "It represents the **expected value of the dependent variable (Y) when all independent variables are equal to zero**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Role of the Intercept in Providing Context**\n",
        "\n",
        "1. **Baseline Value**\n",
        "   - The intercept serves as a **starting or baseline level** of the dependent variable before any effect of the independent variables is applied.\n",
        "\n",
        "2. **Anchoring the Regression Line**\n",
        "   - It determines where the regression line **crosses the Y-axis**, helping define the position of the line in the coordinate system.\n",
        "\n",
        "3. **Understanding the Effect of Predictors**\n",
        "   - By setting a baseline, the intercept allows the slopes to be interpreted as **changes relative to that baseline**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "In a model predicting house prices based on size:\n",
        "\\[\n",
        "Y = 50{,}000 + 3{,}000X\n",
        "\\]\n",
        "\n",
        "- The intercept (**50,000**) represents the estimated house price when the size is zero.\n",
        "- While size = 0 may not be realistic, the intercept still provides a **mathematical reference point** for the relationship.\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Considerations**\n",
        "- The intercept may not always have a **practical real-world meaning**, especially if zero values of predictors are outside the observed data range.\n",
        "- Even in such cases, it is still essential for correctly defining the regression equation.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why the Intercept Matters**\n",
        "- Provides context for interpreting slopes  \n",
        "- Helps understand model predictions  \n",
        "- Ensures correct positioning of the regression line  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "The intercept adds context by acting as a **baseline reference**, allowing the relationship between variables to be interpreted more clearly, even when its real-world meaning is limited.\n"
      ],
      "metadata": {
        "id": "gxckioSRlDTQ"
      },
      "id": "gxckioSRlDTQ"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZoIaEa9glDuF"
      },
      "id": "ZoIaEa9glDuF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 18: What are the limitations of using R² as a sole measure of model performance**\n",
        "\n",
        "Although **R² (coefficient of determination)** is a widely used metric to evaluate regression models, relying on it **alone** can be misleading. It does not always provide a complete or accurate picture of model quality.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Limitations of R²**\n",
        "\n",
        "1. **Does Not Indicate Causation**\n",
        "   - A high R² only shows correlation, not a cause-and-effect relationship.\n",
        "   - Variables may appear related due to coincidence or omitted factors.\n",
        "\n",
        "2. **Always Increases with More Predictors**\n",
        "   - Adding more independent variables will **never decrease R²**, even if those variables are irrelevant.\n",
        "   - This can lead to **overfitting**.\n",
        "\n",
        "3. **Ignores Model Complexity**\n",
        "   - R² does not penalize unnecessary variables.\n",
        "   - Models with many predictors may look better even if they generalize poorly.\n",
        "\n",
        "4. **No Information About Prediction Error**\n",
        "   - A high R² does not guarantee accurate predictions.\n",
        "   - It does not measure how far predictions are from actual values.\n",
        "\n",
        "5. **Insensitive to Bias**\n",
        "   - R² does not reveal whether predictions are systematically overestimated or underestimated.\n",
        "\n",
        "6. **Not Comparable Across Different Datasets**\n",
        "   - R² values cannot be reliably compared across datasets with different target variable distributions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Better Alternatives and Complements**\n",
        "- **Adjusted R²** (accounts for number of predictors)\n",
        "- **RMSE / MAE** (measure prediction error)\n",
        "- **Residual analysis**\n",
        "- **Cross-validation**\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "While R² is useful for understanding how much variance is explained by a model, it should **never be used in isolation**. Combining R² with other evaluation metrics provides a more accurate and reliable assessment of model performance.\n"
      ],
      "metadata": {
        "id": "wurbW33rmP4W"
      },
      "id": "wurbW33rmP4W"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vug5j5gilExh"
      },
      "id": "vug5j5gilExh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Question 19: How would you interpret a large standard error for a regression coefficient**\n",
        "\n",
        "A **large standard error** for a regression coefficient indicates a **high level of uncertainty** in the estimated value of that coefficient.\n",
        "\n",
        "It suggests that the estimated relationship between the independent variable and the dependent variable is **not precise or reliable**.\n",
        "\n",
        "---\n",
        "\n",
        "### **What the Standard Error Represents**\n",
        "\n",
        "- The standard error measures the **variability of the coefficient estimate** across different samples.\n",
        "- A larger standard error means the estimate would change significantly if the model were fitted to another dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Implications of a Large Standard Error**\n",
        "\n",
        "1. **Low Statistical Significance**\n",
        "   - A large standard error usually results in a **small t-statistic**.\n",
        "   - This often leads to a **high p-value**, making the coefficient statistically insignificant.\n",
        "\n",
        "2. **Uncertain Effect Size**\n",
        "   - The true impact of the predictor on the dependent variable is unclear.\n",
        "   - Predictions based on this coefficient may be unreliable.\n",
        "\n",
        "3. **Wide Confidence Intervals**\n",
        "   - Large standard errors produce wide confidence intervals, indicating low precision.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Causes**\n",
        "- Small sample size  \n",
        "- High multicollinearity among predictors  \n",
        "- High variability in the data  \n",
        "- Poor model specification  \n",
        "\n",
        "---\n",
        "\n",
        "### **How to Address Large Standard Errors**\n",
        "- Increase the sample size  \n",
        "- Remove or combine correlated predictors  \n",
        "- Improve model specification  \n",
        "- Standardize or transform variables  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "A large standard error signals that the corresponding regression coefficient should be interpreted with caution, as it reflects **low precision and reduced confidence** in the estimated relationship.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "F3w96vThlFGH"
      },
      "id": "F3w96vThlFGH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a2B8Qb9-lFn8"
      },
      "id": "a2B8Qb9-lFn8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 20: How can heteroscedasticity be identified in residual plots, and why is it important to address it**\n",
        "\n",
        "**Heteroscedasticity** can be identified by analyzing **residual plots**, which display residuals (errors) on the Y-axis against predicted values or an independent variable on the X-axis.\n",
        "\n",
        "Residual plots are a key diagnostic tool in regression analysis.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Identify Heteroscedasticity in Residual Plots**\n",
        "\n",
        "1. **Funnel Shape Pattern**\n",
        "   - Residuals spread out or narrow as X or predicted Y increases.\n",
        "   - Indicates changing variance of errors.\n",
        "\n",
        "2. **Cone or Fan Shape**\n",
        "   - Increasing or decreasing dispersion of residuals across values.\n",
        "   - Clear sign of heteroscedasticity.\n",
        "\n",
        "3. **Uneven Clustering**\n",
        "   - Residuals show larger variance in some regions and smaller in others.\n",
        "\n",
        "4. **Non-Random Pattern**\n",
        "   - Residuals should be randomly scattered around zero.\n",
        "   - Any systematic pattern suggests violation of assumptions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "- If residuals are tightly clustered for small values of X and widely spread for larger values, heteroscedasticity is present.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why It Is Important to Address Heteroscedasticity**\n",
        "\n",
        "1. **Invalid Statistical Inference**\n",
        "   - Standard errors become unreliable.\n",
        "   - Hypothesis tests and confidence intervals may be incorrect.\n",
        "\n",
        "2. **Reduced Model Efficiency**\n",
        "   - The model no longer provides the best linear unbiased estimates.\n",
        "\n",
        "3. **Misleading Conclusions**\n",
        "   - Predictors may appear significant or insignificant incorrectly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Ways to Handle Heteroscedasticity**\n",
        "- Transform the dependent variable (log, square root)\n",
        "- Use weighted least squares\n",
        "- Apply robust standard errors\n",
        "- Improve model specification\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Residual plots provide a simple and effective way to detect heteroscedasticity. Addressing it is crucial to ensure **accurate inference, reliable predictions, and a trustworthy regression model**.\n"
      ],
      "metadata": {
        "id": "6stQsZEolF-a"
      },
      "id": "6stQsZEolF-a"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LaBG1cXmyJo"
      },
      "id": "1LaBG1cXmyJo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 21: What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²**\n",
        "\n",
        "If a **Multiple Linear Regression** model has a **high R²** but a **low adjusted R²**, it indicates that although the model explains a large portion of the variance in the dependent variable, **many of the included independent variables do not contribute meaningfully to the model**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Understanding the Metrics**\n",
        "\n",
        "- **R² (Coefficient of Determination)**\n",
        "  - Measures the proportion of variance in Y explained by all predictors.\n",
        "  - Always increases or stays the same when new variables are added.\n",
        "\n",
        "- **Adjusted R²**\n",
        "  - Adjusts R² by penalizing unnecessary predictors.\n",
        "  - Increases only when a new variable improves the model more than expected by chance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Interpretation of High R² but Low Adjusted R²**\n",
        "\n",
        "1. **Presence of Irrelevant Variables**\n",
        "   - Extra predictors may increase R² but do not genuinely explain Y.\n",
        "   - Adjusted R² decreases because these variables add complexity without value.\n",
        "\n",
        "2. **Overfitting**\n",
        "   - The model fits training data well but may perform poorly on new data.\n",
        "\n",
        "3. **Reduced Model Efficiency**\n",
        "   - Too many predictors reduce interpretability and reliability.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why This Is a Problem**\n",
        "- Misleading model performance\n",
        "- Poor generalization to unseen data\n",
        "- Increased variance in coefficient estimates\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Address This Issue**\n",
        "- Remove insignificant variables\n",
        "- Use feature selection techniques\n",
        "- Rely more on adjusted R² for model comparison\n",
        "- Apply cross-validation\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "A high R² with a low adjusted R² is a warning sign of **overfitting and unnecessary complexity**. Adjusted R² provides a more reliable measure of model quality in Multiple Linear Regression.\n"
      ],
      "metadata": {
        "id": "huyjVLA0myiE"
      },
      "id": "huyjVLA0myiE"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ki3NYD2-my5K"
      },
      "id": "ki3NYD2-my5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 22: Why is it important to scale variables in Multiple Linear Regression**\n",
        "\n",
        "Scaling variables in **Multiple Linear Regression (MLR)** means transforming the independent variables so that they are on a **similar scale**, typically using techniques such as **standardization** or **normalization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Scaling Is Important**\n",
        "\n",
        "1. **Ensures Fair Contribution of Variables**\n",
        "   - When predictors are on very different scales, variables with larger numerical ranges can **dominate the model**, even if they are not more important.\n",
        "\n",
        "2. **Improves Numerical Stability**\n",
        "   - Scaling helps prevent computational issues when estimating regression coefficients, especially with large or very small values.\n",
        "\n",
        "3. **Essential for Regularization Techniques**\n",
        "   - Methods like **Ridge Regression** and **Lasso Regression** are sensitive to scale.\n",
        "   - Without scaling, penalties are applied unevenly, leading to biased results.\n",
        "\n",
        "4. **Helps with Interpretation**\n",
        "   - Standardized coefficients allow comparison of the **relative importance** of predictors.\n",
        "   - Each coefficient represents the effect of a one standard deviation change in the predictor.\n",
        "\n",
        "5. **Reduces Multicollinearity Effects**\n",
        "   - Scaling (especially centering) can reduce multicollinearity caused by interaction or polynomial terms.\n",
        "\n",
        "---\n",
        "\n",
        "### **Common Scaling Techniques**\n",
        "\n",
        "1. **Standardization (Z-score scaling)**\n",
        "\\[\n",
        "X_{scaled} = \\frac{X - \\mu}{\\sigma}\n",
        "\\]\n",
        "   - Mean = 0, Standard Deviation = 1\n",
        "\n",
        "2. **Normalization (Min–Max scaling)**\n",
        "\\[\n",
        "X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
        "\\]\n",
        "   - Scales values between 0 and 1\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "If one variable is measured in **thousands (salary)** and another in **single digits (years of experience)**, scaling ensures both contribute appropriately to the regression model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Scaling variables in Multiple Linear Regression is crucial for **model stability, fair coefficient estimation, proper regularization, and meaningful interpretation**, especially when predictors are measured on different scales.\n"
      ],
      "metadata": {
        "id": "f-yuNVPsmzPo"
      },
      "id": "f-yuNVPsmzPo"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K3zSC31Kmznk"
      },
      "id": "K3zSC31Kmznk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 23: What is polynomial regression**\n",
        "\n",
        "**Polynomial Regression** is a type of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an **nth-degree polynomial** rather than a straight line.\n",
        "\n",
        "It is used when the data shows a **non-linear relationship** that cannot be accurately captured by simple linear regression.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Idea of Polynomial Regression**\n",
        "\n",
        "- Although the relationship between X and Y is non-linear, polynomial regression is still **linear in terms of parameters**.\n",
        "- It extends linear regression by adding **higher-degree terms** of the independent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **General Concept**\n",
        "Instead of fitting a straight line, polynomial regression fits a **curved line** that better follows the pattern of the data.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "If sales increase rapidly at first and then level off with advertising spend, a straight line may not fit well.  \n",
        "Polynomial regression can model this curvature more accurately.\n",
        "\n",
        "---\n",
        "\n",
        "### **Why Polynomial Regression Is Used**\n",
        "- To capture **curved trends** in data  \n",
        "- To improve model fit when linear assumptions fail  \n",
        "- To handle non-linear relationships without switching to complex models  \n",
        "\n",
        "---\n",
        "\n",
        "### **Applications**\n",
        "- Growth curve analysis  \n",
        "- Economics and finance  \n",
        "- Engineering and physical sciences  \n",
        "- Trend modeling in real-world datasets  \n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Polynomial regression is a powerful extension of linear regression that allows modeling of **non-linear relationships** by incorporating polynomial terms, making it useful when data exhibits curvature rather than a straight-line pattern.\n"
      ],
      "metadata": {
        "id": "7rkGYQ4zmz8w"
      },
      "id": "7rkGYQ4zmz8w"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AqaGmW5vm0Tq"
      },
      "id": "AqaGmW5vm0Tq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 24: How does polynomial regression differ from linear regression**\n",
        "\n",
        "Polynomial regression differs from linear regression in the **form of the relationship** it models between the independent variable (X) and the dependent variable (Y).\n",
        "\n",
        "---\n",
        "\n",
        "### **Linear Regression**\n",
        "- Models a **straight-line relationship** between X and Y.\n",
        "- Assumes the change in Y is constant for every unit change in X.\n",
        "- Equation:\n",
        "\\[\n",
        "Y = mX + c\n",
        "\\]\n",
        "\n",
        "- Suitable when data shows a **linear trend**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Polynomial Regression**\n",
        "- Models a **non-linear (curved) relationship** using polynomial terms of X.\n",
        "- Allows the rate of change in Y to vary with X.\n",
        "- General equation:\n",
        "\\[\n",
        "Y = b_0 + b_1X + b_2X^2 + \\cdots + b_nX^n\n",
        "\\]\n",
        "\n",
        "- Suitable when data exhibits **curvature or complex patterns**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Differences**\n",
        "\n",
        "| Aspect | Linear Regression | Polynomial Regression |\n",
        "|------|------------------|----------------------|\n",
        "| Relationship type | Linear | Non-linear |\n",
        "| Model flexibility | Low | Higher |\n",
        "| Fit to curved data | Poor | Good |\n",
        "| Risk of overfitting | Low | Higher (with high degree) |\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Note**\n",
        "Although polynomial regression models non-linear relationships, it is still **linear in terms of coefficients**, allowing it to be solved using linear regression techniques.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Linear regression is best for simple, straight-line relationships, while polynomial regression is used when the data shows **non-linear trends** that a straight line cannot capture effectively.\n"
      ],
      "metadata": {
        "id": "k577TkkKm0qO"
      },
      "id": "k577TkkKm0qO"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ynFlNv56m1PR"
      },
      "id": "ynFlNv56m1PR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 25: When is polynomial regression used**\n",
        "\n",
        "Polynomial regression is used when the relationship between the independent variable (X) and the dependent variable (Y) is **non-linear** and cannot be accurately modeled using a straight line.\n",
        "\n",
        "It is especially useful when data shows **curvature or changing trends**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Situations Where Polynomial Regression Is Used**\n",
        "\n",
        "1. **Non-Linear Data Patterns**\n",
        "   - When scatter plots show a curved or parabolic trend rather than a straight line.\n",
        "\n",
        "2. **Changing Rate of Growth or Decline**\n",
        "   - When the effect of X on Y increases or decreases at different levels of X.\n",
        "\n",
        "3. **Improving Model Fit**\n",
        "   - When linear regression underfits the data and produces large systematic errors.\n",
        "\n",
        "4. **Physical and Natural Phenomena**\n",
        "   - Common in physics, engineering, and biology where relationships are often non-linear.\n",
        "\n",
        "5. **Trend Analysis**\n",
        "   - Used to model trends such as population growth, sales patterns, or learning curves.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "- Predicting fuel efficiency based on speed, where efficiency increases up to a point and then decreases.\n",
        "- Modeling profit vs. advertising spend where returns diminish after a certain level.\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Considerations**\n",
        "- The degree of the polynomial should be chosen carefully.\n",
        "- Higher-degree polynomials may lead to **overfitting**.\n",
        "- Visualization and validation are essential.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Polynomial regression is used when data exhibits **non-linear behavior**, allowing the model to capture complex patterns that linear regression cannot represent effectively.\n"
      ],
      "metadata": {
        "id": "Rdn0Kgdnm1ec"
      },
      "id": "Rdn0Kgdnm1ec"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y00F1VLTm2EE"
      },
      "id": "Y00F1VLTm2EE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 26: What is the general equation for polynomial regression**\n",
        "\n",
        "The **general equation for polynomial regression** expresses the dependent variable (Y) as a polynomial function of the independent variable (X), allowing the model to capture **non-linear relationships**.\n",
        "\n",
        "---\n",
        "\n",
        "### **General Polynomial Regression Equation**\n",
        "\n",
        "\\[\n",
        "Y = b_0 + b_1X + b_2X^2 + b_3X^3 + \\cdots + b_nX^n + \\varepsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **Y** → Dependent (response) variable  \n",
        "- **X** → Independent (predictor) variable  \n",
        "- **b₀** → Intercept  \n",
        "- **b₁, b₂, …, bₙ** → Regression coefficients  \n",
        "- **n** → Degree of the polynomial  \n",
        "- **ε** → Error term  \n",
        "\n",
        "---\n",
        "\n",
        "### **Explanation of the Terms**\n",
        "- **b₀ (Intercept):** Baseline value of Y when X = 0  \n",
        "- **Polynomial terms (X², X³, …):** Capture curvature and non-linear trends  \n",
        "- **Degree (n):**\n",
        "  - n = 1 → Linear regression  \n",
        "  - n = 2 → Quadratic regression  \n",
        "  - n = 3 → Cubic regression  \n",
        "\n",
        "---\n",
        "\n",
        "### **Important Characteristics**\n",
        "- Polynomial regression is **non-linear in X**, but **linear in parameters (coefficients)**.\n",
        "- This allows it to be solved using ordinary least squares methods.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "A quadratic regression model:\n",
        "\\[\n",
        "Y = 2 + 3X + 1.5X^2\n",
        "\\]\n",
        "\n",
        "This model can represent a **parabolic relationship** between X and Y.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "The general equation of polynomial regression extends linear regression by including higher-order powers of X, enabling the model to represent **complex, curved relationships** while remaining mathematically tractable.\n"
      ],
      "metadata": {
        "id": "DpaU_srsm2TY"
      },
      "id": "DpaU_srsm2TY"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sztV3I6vn7n_"
      },
      "id": "sztV3I6vn7n_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 27: Can polynomial regression be applied to multiple variables**\n",
        "\n",
        "Yes, **polynomial regression can be applied to multiple variables**. This is known as **multivariate polynomial regression**, where the dependent variable (Y) is modeled using polynomial terms of **two or more independent variables**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Multivariate Polynomial Regression**\n",
        "\n",
        "In this approach, polynomial terms are created not only for each independent variable but also for their **interaction terms**, allowing the model to capture complex non-linear relationships.\n",
        "\n",
        "---\n",
        "\n",
        "### **General Equation**\n",
        "\n",
        "\\[\n",
        "Y = b_0 + b_1X_1 + b_2X_2 + b_3X_1^2 + b_4X_2^2 + b_5(X_1X_2) + \\cdots + \\varepsilon\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- **X₁, X₂** → Independent variables  \n",
        "- **X₁², X₂²** → Polynomial (squared) terms  \n",
        "- **X₁X₂** → Interaction term  \n",
        "- **bᵢ** → Regression coefficients  \n",
        "\n",
        "---\n",
        "\n",
        "### **Why Use Polynomial Regression with Multiple Variables**\n",
        "\n",
        "- To model **non-linear effects** of multiple predictors  \n",
        "- To capture **interaction effects** between variables  \n",
        "- To improve model accuracy when linear assumptions fail  \n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "Predicting house prices using:\n",
        "- Size of the house (X₁)\n",
        "- Age of the house (X₂)\n",
        "\n",
        "Polynomial regression can model:\n",
        "- Non-linear effect of size and age\n",
        "- Interaction between size and age on price\n",
        "\n",
        "---\n",
        "\n",
        "### **Important Considerations**\n",
        "- The number of features increases rapidly with polynomial degree.\n",
        "- High-degree models may lead to **overfitting**.\n",
        "- Feature scaling is often necessary.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Polynomial regression can be effectively applied to multiple variables, enabling the modeling of **complex, non-linear, and interaction-driven relationships**, but it must be used carefully to balance flexibility and generalization.\n"
      ],
      "metadata": {
        "id": "Elwrdddan8Qk"
      },
      "id": "Elwrdddan8Qk"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x_z-OtZColau"
      },
      "id": "x_z-OtZColau",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 28: What are the limitations of polynomial regression**\n",
        "\n",
        "While **polynomial regression** is useful for modeling non-linear relationships, it has several limitations that must be considered to avoid incorrect conclusions and poor model performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Key Limitations of Polynomial Regression**\n",
        "\n",
        "1. **Risk of Overfitting**\n",
        "   - Higher-degree polynomials can fit the training data very closely.\n",
        "   - This may capture noise instead of the true pattern, leading to poor performance on new data.\n",
        "\n",
        "2. **Poor Extrapolation**\n",
        "   - Polynomial models can behave unpredictably outside the range of observed data.\n",
        "   - Predictions beyond the data range may be unrealistic or extreme.\n",
        "\n",
        "3. **Increased Model Complexity**\n",
        "   - As the degree increases, the model becomes more complex and harder to interpret.\n",
        "   - Coefficients lose intuitive meaning.\n",
        "\n",
        "4. **Multicollinearity**\n",
        "   - Polynomial terms (X, X², X³, etc.) are often highly correlated.\n",
        "   - This can inflate standard errors and make coefficient estimates unstable.\n",
        "\n",
        "5. **Sensitivity to Outliers**\n",
        "   - Polynomial regression is highly sensitive to extreme values.\n",
        "   - Outliers can significantly distort the fitted curve.\n",
        "\n",
        "6. **Computational Cost**\n",
        "   - High-degree polynomials with multiple variables can lead to a large number of features.\n",
        "   - This increases computation time and memory usage.\n",
        "\n",
        "---\n",
        "\n",
        "### **When to Be Careful**\n",
        "- Small datasets  \n",
        "- High-degree polynomial selection  \n",
        "- Extrapolation beyond observed values  \n",
        "\n",
        "---\n",
        "\n",
        "### **Ways to Mitigate Limitations**\n",
        "- Use cross-validation to select polynomial degree\n",
        "- Apply regularization techniques\n",
        "- Limit polynomial degree\n",
        "- Visualize model fit\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Polynomial regression is powerful for capturing non-linear trends, but its limitations—especially **overfitting, poor extrapolation, and interpretability issues**—require careful model selection and validation.\n"
      ],
      "metadata": {
        "id": "gFz4bf-EolxA"
      },
      "id": "gFz4bf-EolxA"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wjYqvc31omM8"
      },
      "id": "wjYqvc31omM8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 29: What methods can be used to evaluate model fit when selecting the degree of a polynomial**\n",
        "\n",
        "Selecting the appropriate **degree of a polynomial** is crucial in polynomial regression to balance **underfitting and overfitting**. Several evaluation methods are used to assess model fit and choose the optimal degree.\n",
        "\n",
        "---\n",
        "\n",
        "### **Methods to Evaluate Model Fit**\n",
        "\n",
        "1. **Visual Inspection (Plotting the Fit)**\n",
        "   - Plot the regression curve against the data points.\n",
        "   - Helps identify:\n",
        "     - Underfitting (too simple, misses patterns)\n",
        "     - Overfitting (too complex, follows noise)\n",
        "\n",
        "2. **R² and Adjusted R²**\n",
        "   - **R²** increases with higher degrees but can be misleading.\n",
        "   - **Adjusted R²** penalizes unnecessary complexity and is preferred when comparing polynomial degrees.\n",
        "\n",
        "3. **Mean Squared Error (MSE) / Root Mean Squared Error (RMSE)**\n",
        "   - Measures average prediction error.\n",
        "   - Lower values indicate better fit.\n",
        "   - RMSE is easier to interpret as it is in the same units as Y.\n",
        "\n",
        "4. **Cross-Validation**\n",
        "   - Splits data into training and validation sets.\n",
        "   - Evaluates model performance on unseen data.\n",
        "   - Helps select a degree that generalizes well.\n",
        "\n",
        "5. **Residual Analysis**\n",
        "   - Residuals should be randomly scattered around zero.\n",
        "   - Patterns indicate underfitting or overfitting.\n",
        "\n",
        "6. **Bias–Variance Tradeoff**\n",
        "   - Low degree → High bias, low variance (underfitting)\n",
        "   - High degree → Low bias, high variance (overfitting)\n",
        "   - Optimal degree balances both.\n",
        "\n",
        "7. **Information Criteria**\n",
        "   - **AIC (Akaike Information Criterion)**\n",
        "   - **BIC (Bayesian Information Criterion)**\n",
        "   - Penalize model complexity more explicitly.\n",
        "\n",
        "---\n",
        "\n",
        "### **Best Practice**\n",
        "- Combine **cross-validation**, **error metrics**, and **visualization**\n",
        "- Avoid relying on a single metric\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Evaluating model fit when selecting the polynomial degree requires a combination of **quantitative metrics and visual diagnostics**. Proper evaluation ensures a model that is both accurate and generalizable.\n"
      ],
      "metadata": {
        "id": "sXmn_4MjomeK"
      },
      "id": "sXmn_4MjomeK"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WPq7QM0ZonNy"
      },
      "id": "WPq7QM0ZonNy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 30: Why is visualization important in polynomial regression**\n",
        "\n",
        "Visualization plays a crucial role in **polynomial regression** because it helps understand, evaluate, and validate the **non-linear relationship** between the independent variable(s) and the dependent variable.\n",
        "\n",
        "---\n",
        "\n",
        "### **Importance of Visualization in Polynomial Regression**\n",
        "\n",
        "1. **Understanding Data Patterns**\n",
        "   - Visual plots reveal whether the relationship between variables is linear or non-linear.\n",
        "   - Helps decide whether polynomial regression is appropriate.\n",
        "\n",
        "2. **Selecting the Polynomial Degree**\n",
        "   - Visualization helps detect:\n",
        "     - **Underfitting** (curve too simple to capture the pattern)\n",
        "     - **Overfitting** (curve too complex and follows noise)\n",
        "   - Enables choosing a degree that best represents the true trend.\n",
        "\n",
        "3. **Model Validation**\n",
        "   - Comparing predicted curves with actual data points shows how well the model fits.\n",
        "   - Makes it easier to identify regions where the model performs poorly.\n",
        "\n",
        "4. **Residual Analysis**\n",
        "   - Plotting residuals helps check:\n",
        "     - Randomness\n",
        "     - Homoscedasticity\n",
        "     - Presence of systematic patterns\n",
        "   - Essential for verifying regression assumptions.\n",
        "\n",
        "5. **Detecting Outliers and Influential Points**\n",
        "   - Visualization highlights outliers that can disproportionately affect the polynomial curve.\n",
        "\n",
        "6. **Improving Interpretability**\n",
        "   - Polynomial equations can be complex.\n",
        "   - Visual plots make results easier to explain to non-technical stakeholders.\n",
        "\n",
        "---\n",
        "\n",
        "### **Example**\n",
        "- A scatter plot with a fitted polynomial curve can clearly show whether a quadratic or cubic model better captures the trend compared to a straight line.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "Visualization is essential in polynomial regression because it supports **model selection, diagnosis, interpretation, and communication**, ensuring the chosen model accurately represents the underlying data without unnecessary complexity.\n"
      ],
      "metadata": {
        "id": "6cKFP2hEong4"
      },
      "id": "6cKFP2hEong4"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mpAqkPx9on42"
      },
      "id": "mpAqkPx9on42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Question 31: How is polynomial regression implemented in Python?**\n",
        "\n",
        "Polynomial regression in Python is typically implemented using **NumPy** and **scikit-learn** by transforming the input features into polynomial features and then applying **linear regression**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Steps to Implement Polynomial Regression in Python**\n",
        "\n",
        "1. **Import Required Libraries**\n",
        "   - NumPy for numerical operations\n",
        "   - scikit-learn for model building\n",
        "   - Matplotlib for visualization\n",
        "\n",
        "2. **Prepare the Dataset**\n",
        "   - Define the independent variable (X) and dependent variable (Y)\n",
        "   - Reshape X to a 2D array as required by scikit-learn\n",
        "\n",
        "3. **Create Polynomial Features**\n",
        "   - Use `PolynomialFeatures` to generate higher-degree terms\n",
        "\n",
        "4. **Fit a Linear Regression Model**\n",
        "   - Train the model using transformed polynomial features\n",
        "\n",
        "5. **Make Predictions**\n",
        "   - Predict values using the trained model\n",
        "\n",
        "6. **Visualize the Results**\n",
        "   - Plot the polynomial curve along with actual data points\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "cexJ2atXooPX"
      },
      "id": "cexJ2atXooPX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Sample data\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2, 4, 5, 4, 6])\n",
        "\n",
        "# Create polynomial features (degree = 2)\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "# Train the model\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "# Predict values\n",
        "Y_pred = model.predict(X_poly)\n",
        "\n",
        "# Visualization\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X, Y_pred)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Polynomial Regression (Degree 2)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "ywrIC-kypcX4",
        "outputId": "abdd0cf5-ce22-465a-d379-5a50fd47c4a8"
      },
      "id": "ywrIC-kypcX4",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bqwXf86oqf4J"
      },
      "id": "bqwXf86oqf4J",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}