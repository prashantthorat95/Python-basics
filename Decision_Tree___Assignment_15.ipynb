{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Decision Tree | Assignment**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k_Sdk_kPWa_Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 1: What is a Decision Tree, and how does it work in the context of classification?**\n",
        "\n",
        "### 1. Conceptual Explanation\n",
        "A Decision Tree is a supervised machine learning algorithm that splits data into branches based on feature values to make predictions.  \n",
        "In classification, it assigns a class label by traversing from root to leaf using decision rules.\n",
        "\n",
        "### 2. Mathematical Explanation\n",
        "At each node, we choose a feature that maximizes Information Gain:\n",
        "\n",
        "Information Gain = Parent Impurity − Weighted Child Impurity\n",
        "\n",
        "Using Entropy:\n",
        "Entropy(S) = − Σ pᵢ log₂(pᵢ)\n",
        "\n",
        "Split chosen:\n",
        "IG(S, A) = Entropy(S) − Σ (|Sᵥ| / |S|) Entropy(Sᵥ)\n",
        "\n",
        "### 3. Intuitive Explanation\n",
        "Like a flowchart:\n",
        "\"Is age > 30?\" → Yes/No  \n",
        "\"Is income high?\" → Yes/No  \n",
        "Finally, reach a decision like “Approve Loan” or “Reject Loan”.\n",
        "\n",
        "### 4. Real-World Example\n",
        "In healthcare:\n",
        "Root: Is blood pressure high?\n",
        "Branch: Is sugar level high?\n",
        "Leaf: Patient has heart disease (Yes/No)\n",
        "\n",
        "### 5. Assumptions\n",
        "- Data can be split using clear rules\n",
        "- Features are informative\n",
        "- Training data represents real-world patterns\n",
        "\n",
        "### 6. Advantages\n",
        "- Easy to interpret\n",
        "- Handles both numeric & categorical data\n",
        "- No feature scaling needed\n",
        "\n",
        "### 7. Disadvantages\n",
        "- Overfitting\n",
        "- Sensitive to small data changes\n",
        "- Greedy splitting\n",
        "\n",
        "### 8. Use Cases\n",
        "- Medical diagnosis\n",
        "- Credit approval\n",
        "- Customer churn prediction\n"
      ],
      "metadata": {
        "id": "X-IzO_qZWVxu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HSYEg_dYaE3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?**\n",
        "\n",
        "### 1. Definition (Conceptual)\n",
        "\n",
        "In a Decision Tree, impurity measures tell us how “mixed” the classes are at a node.  \n",
        "A node is:\n",
        "- **Pure** → all samples belong to one class  \n",
        "- **Impure** → samples belong to multiple classes  \n",
        "\n",
        "Two most common impurity measures are:\n",
        "\n",
        "#### (a) Gini Impurity  \n",
        "Gini Impurity measures the probability that a randomly chosen sample from a node would be **incorrectly classified** if it were randomly labeled according to the class distribution in that node.\n",
        "\n",
        "#### (b) Entropy  \n",
        "Entropy measures the **amount of uncertainty or disorder** in the node. It comes from Information Theory and quantifies how unpredictable the class labels are.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Mathematical Definitions\n",
        "\n",
        "Let a node contain samples from \\(k\\) classes, and  \n",
        "\\(p_i\\) = proportion of samples of class \\(i\\).\n",
        "\n",
        "**Gini Impurity**\n",
        "\n",
        "\\[\n",
        "Gini = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "\\]\n",
        "\n",
        "- If all samples belong to one class → Gini = 0 (pure)\n",
        "- If classes are evenly mixed → Gini is maximum\n",
        "\n",
        "**Entropy**\n",
        "\n",
        "\\[\n",
        "Entropy = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
        "\\]\n",
        "\n",
        "- If one class has probability 1 → Entropy = 0 (no uncertainty)\n",
        "- If classes are equally likely → Entropy is maximum\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Intuitive Explanation\n",
        "\n",
        "Think of a basket of fruits:\n",
        "- Basket with only apples → very pure → low Gini, low Entropy  \n",
        "- Basket with 50% apples and 50% oranges → very mixed → high Gini, high Entropy  \n",
        "\n",
        "A Decision Tree tries to ask questions (splits) so that each resulting basket contains mostly one type of fruit.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Impact on Splits\n",
        "\n",
        "For every possible split, the tree computes the impurity reduction:\n",
        "\n",
        "\\[\n",
        "\\text{Impurity Reduction} = \\text{Impurity(parent)} - \\sum \\left( \\frac{n_{child}}{n_{parent}} \\times \\text{Impurity(child)} \\right)\n",
        "\\]\n",
        "\n",
        "The split that gives the **maximum reduction in impurity** is chosen.\n",
        "\n",
        "- Gini → chooses split that minimizes misclassification probability  \n",
        "- Entropy → chooses split that maximally reduces uncertainty  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. Real-World Example\n",
        "\n",
        "In email spam detection:\n",
        "- A node with 90% spam and 10% non-spam has low impurity.\n",
        "- A node with 50% spam and 50% non-spam has high impurity.\n",
        "\n",
        "A good feature (like the word “free”) creates child nodes that are more pure, thus lowering Gini and Entropy.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Comparison\n",
        "\n",
        "| Aspect | Gini Impurity | Entropy |\n",
        "|--------|---------------|---------|\n",
        "| Formula | \\(1 - \\sum p_i^2\\) | \\(-\\sum p_i \\log_2 p_i\\) |\n",
        "| Speed | Faster | Slightly slower |\n",
        "| Interpretation | Misclassification probability | Uncertainty / information |\n",
        "| Used In | CART | ID3, C4.5 |\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Advantages\n",
        "- Clearly measures node purity\n",
        "- Helps in selecting the best feature for splitting\n",
        "- Works for multi-class classification\n",
        "\n",
        "### 8. Disadvantages\n",
        "- Greedy (local optimum)\n",
        "- Sensitive to noise\n",
        "- Biased towards features with many categories\n",
        "\n",
        "### 9. Use Cases\n",
        "- Decision Tree classifiers\n",
        "- Random Forests\n",
        "- Medical diagnosis systems\n",
        "- Credit risk assessment\n"
      ],
      "metadata": {
        "id": "43WW1hEEaiNq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b05zRRL0arYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.**\n",
        "\n",
        "### 1. Conceptual Definition\n",
        "\n",
        "In Decision Trees, **pruning** is used to control the size of the tree and avoid overfitting.\n",
        "\n",
        "There are two main types:\n",
        "\n",
        "### (a) Pre-Pruning (Early Stopping)\n",
        "Pre-pruning stops the tree from growing further **during training** by setting constraints such as:\n",
        "- Maximum depth of the tree\n",
        "- Minimum number of samples in a node\n",
        "- Minimum information gain for a split\n",
        "\n",
        "The tree is prevented from becoming too complex.\n",
        "\n",
        "### (b) Post-Pruning (Backward Pruning)\n",
        "Post-pruning allows the tree to grow **fully first**, and then removes (cuts) branches that do not improve performance on validation data.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Mathematical View\n",
        "\n",
        "Pre-pruning conditions:\n",
        "\\[\n",
        "\\text{Stop splitting if: } depth \\ge d_{max} \\quad \\text{or} \\quad samples < n_{min}\n",
        "\\]\n",
        "\n",
        "Post-pruning uses cost-complexity:\n",
        "\\[\n",
        "Cost(T) = Error(T) + \\alpha \\times \\text{Number of Leaves}\n",
        "\\]\n",
        "\n",
        "where:\n",
        "- \\(Error(T)\\) = misclassification error\n",
        "- \\(\\alpha\\) = complexity penalty\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Intuitive Explanation\n",
        "\n",
        "Think of growing a plant:\n",
        "\n",
        "- **Pre-pruning**: You stop the plant from growing too tall early.\n",
        "- **Post-pruning**: You let it grow fully, then trim unnecessary branches.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Real-World Example\n",
        "\n",
        "In a **loan approval system**:\n",
        "\n",
        "- Pre-pruning: Limit tree depth so that rules remain simple and fast.\n",
        "- Post-pruning: Build a complex rule system first, then remove conditions that do not improve accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Practical Advantages\n",
        "\n",
        "#### Advantage of Pre-Pruning\n",
        "✅ **Faster training and simpler model**\n",
        "\n",
        "Example:  \n",
        "In real-time fraud detection, decisions must be made in milliseconds. A small pre-pruned tree gives quick predictions with acceptable accuracy.\n",
        "\n",
        "#### Advantage of Post-Pruning\n",
        "✅ **Higher predictive accuracy**\n",
        "\n",
        "Example:  \n",
        "In medical diagnosis, capturing complex interactions is important. Post-pruning keeps useful deep patterns and removes only truly irrelevant branches.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Comparison Table\n",
        "\n",
        "| Aspect | Pre-Pruning | Post-Pruning |\n",
        "|--------|------------|--------------|\n",
        "| When applied | During training | After full training |\n",
        "| Tree size | Smaller | Initially large, then reduced |\n",
        "| Risk | Underfitting | Overfitting (before pruning) |\n",
        "| Accuracy | Moderate | Usually higher |\n",
        "| Computation | Low | Higher |\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Advantages\n",
        "\n",
        "**Pre-Pruning**\n",
        "- Prevents overfitting early\n",
        "- Fast and memory efficient\n",
        "- Easy to control\n",
        "\n",
        "**Post-Pruning**\n",
        "- Better generalization\n",
        "- Keeps important complex patterns\n",
        "- More accurate on unseen data\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Disadvantages\n",
        "\n",
        "**Pre-Pruning**\n",
        "- May stop too early → underfitting\n",
        "\n",
        "**Post-Pruning**\n",
        "- Computationally expensive\n",
        "- Requires validation set\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Use Cases\n",
        "\n",
        "- Pre-Pruning: Large-scale real-time systems (banking, recommendation)\n",
        "- Post-Pruning: High-accuracy systems (healthcare, fault diagnosis)\n"
      ],
      "metadata": {
        "id": "lczNy5j8asCc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZjstYNqibhXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?**\n",
        "\n",
        "### 1. Conceptual Definition\n",
        "\n",
        "**Information Gain (IG)** is a measure used in Decision Trees to determine how well a feature separates the training data into target classes.  \n",
        "It tells us how much **uncertainty (entropy)** is reduced after splitting the data using a particular feature.\n",
        "\n",
        "In simple words:  \n",
        "> Information Gain measures how much “information” a feature gives us about the class label.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Mathematical Formulation\n",
        "\n",
        "Let:\n",
        "- \\(S\\) = parent dataset\n",
        "- \\(A\\) = attribute (feature) used for splitting\n",
        "- \\(S_v\\) = subset of \\(S\\) where attribute \\(A\\) takes value \\(v\\)\n",
        "\n",
        "#### Entropy of dataset \\(S\\):\n",
        "\n",
        "\\[\n",
        "Entropy(S) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
        "\\]\n",
        "\n",
        "where \\(p_i\\) is the proportion of class \\(i\\).\n",
        "\n",
        "#### Information Gain:\n",
        "\n",
        "\\[\n",
        "IG(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} Entropy(S_v)\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Intuitive Explanation\n",
        "\n",
        "Imagine you are playing the game **“Guess the Animal”**:\n",
        "\n",
        "You want to ask the question that reduces your uncertainty the most.\n",
        "\n",
        "- Bad question: “Is it an animal?” (almost no information)\n",
        "- Good question: “Does it have feathers?” (big reduction in possibilities)\n",
        "\n",
        "Information Gain chooses the feature that gives the **most clarity** in one step.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. How It Affects Splits\n",
        "\n",
        "For every feature:\n",
        "1. Calculate entropy before split.\n",
        "2. Calculate weighted entropy after split.\n",
        "3. Compute Information Gain.\n",
        "4. Choose the feature with **maximum IG**.\n",
        "\n",
        "So, the best split is:\n",
        "\n",
        "\\[\n",
        "\\text{Best Feature} = \\arg\\max_A IG(S, A)\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Real-World Example\n",
        "\n",
        "In **email spam detection**:\n",
        "\n",
        "Features:\n",
        "- Contains word “free”\n",
        "- Contains link\n",
        "- Sender is unknown\n",
        "\n",
        "The feature “Contains word ‘free’” might reduce uncertainty the most, so it gets the highest Information Gain and becomes the root split.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Assumptions\n",
        "\n",
        "- Data is representative.\n",
        "- Entropy correctly measures uncertainty.\n",
        "- Features are independent for splitting decisions.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Advantages\n",
        "\n",
        "- Provides a clear criterion for feature selection.\n",
        "- Produces informative and interpretable trees.\n",
        "- Based on solid information theory.\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Disadvantages\n",
        "\n",
        "- Biased toward attributes with many unique values.\n",
        "- Greedy (chooses local best split, not global).\n",
        "- Sensitive to noise.\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Use Cases\n",
        "\n",
        "- Decision Tree classifiers (ID3, C4.5)\n",
        "- Feature selection\n",
        "- Medical diagnosis systems\n",
        "- Credit risk modeling\n",
        "- Customer churn prediction\n"
      ],
      "metadata": {
        "id": "kejc4kMebhu5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zhI30g4ab8iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?**\n",
        "\n",
        "### 1. Conceptual Explanation\n",
        "\n",
        "A **Decision Tree** is a rule-based machine learning model that makes decisions by splitting data into branches based on feature conditions.  \n",
        "Because of its tree-like and human-readable structure, it is widely used in real-world decision-making systems.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Common Real-World Applications\n",
        "\n",
        "#### (a) Healthcare\n",
        "- Disease diagnosis (diabetes, cancer, heart disease)\n",
        "- Patient risk classification\n",
        "- Treatment recommendation systems  \n",
        "\n",
        "#### (b) Finance\n",
        "- Loan approval and credit scoring\n",
        "- Fraud detection\n",
        "- Stock market trend classification  \n",
        "\n",
        "#### (c) Marketing\n",
        "- Customer segmentation\n",
        "- Churn prediction\n",
        "- Targeted advertising  \n",
        "\n",
        "#### (d) Retail & E-commerce\n",
        "- Product recommendation\n",
        "- Demand forecasting\n",
        "- Inventory classification  \n",
        "\n",
        "#### (e) Manufacturing\n",
        "- Fault detection\n",
        "- Quality control\n",
        "- Predictive maintenance  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 4. Intuitive Explanation\n",
        "\n",
        "A Decision Tree works like a **doctor's decision process**:\n",
        "\n",
        "- If fever > 100°F → Check cough  \n",
        "- If cough = yes → Check chest pain  \n",
        "- Final decision → Disease present or not  \n",
        "\n",
        "Each question reduces uncertainty and leads to a final decision.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Advantages\n",
        "\n",
        "#### (a) Easy to Understand and Interpret\n",
        "- Can be visualized as flowcharts\n",
        "- Useful in domains requiring explainability (healthcare, law, banking)\n",
        "\n",
        "#### (b) Handles Non-Linear Relationships\n",
        "- No need for linear assumptions\n",
        "\n",
        "#### (c) Minimal Data Preprocessing\n",
        "- No scaling required\n",
        "- Handles categorical and numerical features\n",
        "\n",
        "#### (d) Feature Importance\n",
        "- Automatically identifies important variables\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Limitations\n",
        "\n",
        "#### (a) Overfitting\n",
        "- Deep trees memorize noise\n",
        "- Poor generalization\n",
        "\n",
        "#### (b) Instability\n",
        "- Small data change → different tree\n",
        "\n",
        "#### (c) Greedy Nature\n",
        "- Finds local optimal splits, not global\n",
        "\n",
        "#### (d) Bias Toward Multi-valued Attributes\n",
        "- Information Gain favors high-cardinality features\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Assumptions\n",
        "\n",
        "- Features contain useful decision rules\n",
        "- Training data represents real population\n",
        "- Splits reduce class impurity\n",
        "\n",
        "---\n",
        "\n",
        "### 8. Practical Use Case Example\n",
        "\n",
        "**Loan Approval System**\n",
        "\n",
        "Rules:\n",
        "- If income > 50k → Low risk  \n",
        "- If credit score > 700 → Approve  \n",
        "- Else → Reject  \n",
        "\n",
        "This tree:\n",
        "- Is interpretable to bank managers\n",
        "- Justifies decisions legally\n",
        "- Helps reduce default risk\n",
        "\n",
        "---\n",
        "\n",
        "### 9. Summary Table\n",
        "\n",
        "| Aspect | Details |\n",
        "|--------|--------|\n",
        "| Strengths | Interpretable, non-linear, fast |\n",
        "| Weaknesses | Overfitting, unstable |\n",
        "| Best Used In | Healthcare, Finance, Marketing |\n",
        "| Not Ideal For | Very noisy, high-dimensional data |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ABkSxmr1b755"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YY4C5fVYcUJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Question 6: Write a Python program to load the**\n",
        "(1) Iris dataset, train a Decision Tree Classifier using the Gini criterion, and print the model’s accuracy and feature importances.\n",
        "\n"
      ],
      "metadata": {
        "id": "wf7FkETocVBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier using Gini criterion\n",
        "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    \"Feature\": iris.feature_names,\n",
        "    \"Importance\": dt_model.feature_importances_\n",
        "})\n",
        "\n",
        "# Print accuracy and feature importances\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zLBQgC1cVXh",
        "outputId": "f2bd1070-30b9-4a5a-d8b3-1921fc50e0d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.000000\n",
            "1   sepal width (cm)    0.019110\n",
            "2  petal length (cm)    0.893264\n",
            "3   petal width (cm)    0.087626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XvVxw2cGkDi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 7: Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "(Include your Python code and"
      ],
      "metadata": {
        "id": "0FhicMmXcVmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train fully-grown Decision Tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_tree_pred = full_tree.predict(X_test)\n",
        "full_tree_accuracy = accuracy_score(y_test, full_tree_pred)\n",
        "\n",
        "# Train Decision Tree with max_depth = 3\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "pruned_tree_pred = pruned_tree.predict(X_test)\n",
        "pruned_tree_accuracy = accuracy_score(y_test, pruned_tree_pred)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Fully-Grown Tree Accuracy:\", full_tree_accuracy)\n",
        "print(\"Max Depth = 3 Tree Accuracy:\", pruned_tree_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Wj3nCskGds",
        "outputId": "99fda80e-868f-4231-ef25-9cb0f626e14c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-Grown Tree Accuracy: 1.0\n",
            "Max Depth = 3 Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U7nDe3Y8k6Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 8: Write a Python program to:**\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "SJBBPxsak6s6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOJHVz6fmABt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load Boston Housing Dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Decision Tree Regressor\n",
        "dt_reg = DecisionTreeRegressor(random_state=42)\n",
        "dt_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = dt_reg.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pd.DataFrame({\n",
        "    \"Feature\": boston.feature_names,\n",
        "    \"Importance\": dt_reg.feature_importances_\n",
        "})\n",
        "\n",
        "# Print results\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importances)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLdX9Ai0k7Kf",
        "outputId": "018a8758-bb28-4109-fecc-622f797801b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 11.588026315789474\n",
            "\n",
            "Feature Importances:\n",
            "    Feature  Importance\n",
            "0      CRIM    0.058465\n",
            "1        ZN    0.000989\n",
            "2     INDUS    0.009872\n",
            "3      CHAS    0.000297\n",
            "4       NOX    0.007051\n",
            "5        RM    0.575807\n",
            "6       AGE    0.007170\n",
            "7       DIS    0.109624\n",
            "8       RAD    0.001646\n",
            "9       TAX    0.002181\n",
            "10  PTRATIO    0.025043\n",
            "11        B    0.011873\n",
            "12    LSTAT    0.189980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xg3dFD7GmENE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Question 9: Write a Python program to:**\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "nrKvNoQLmFzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize Decision Tree Classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(dt, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions with best model\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print best parameters and accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Model Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZjZ2jEvmFNH",
        "outputId": "f6fee955-d44c-42ab-f2ce-a4898bfe2ee5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Best Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9aQJyruBnPYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Question 10: End-to-End Process for Disease Prediction Using a Decision Tree**\n",
        "\n",
        "You are working as a data scientist in a healthcare company. The goal is to predict whether a patient has a certain disease using a dataset that contains:\n",
        "- Numerical features (age, blood pressure, sugar level, etc.)\n",
        "- Categorical features (gender, smoking status, symptom type, etc.)\n",
        "- Missing values\n",
        "\n",
        "Below is the complete step-by-step process.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Handling Missing Values\n",
        "\n",
        "#### Conceptual\n",
        "Medical datasets often have missing entries due to:\n",
        "- Patients skipping tests\n",
        "- Data entry errors\n",
        "- Sensor failures\n",
        "\n",
        "Decision Trees cannot handle missing values directly, so we must impute them.\n",
        "\n",
        "#### Methods\n",
        "- **Numerical Features**\n",
        "  - Mean Imputation: replace missing value with average\n",
        "  - Median Imputation: robust to outliers\n",
        "- **Categorical Features**\n",
        "  - Mode Imputation (most frequent category)\n",
        "\n",
        "#### Mathematical\n",
        "Mean Imputation:\n",
        "\\[\n",
        "x_{missing} = \\frac{1}{n}\\sum_{i=1}^{n} x_i\n",
        "\\]\n",
        "\n",
        "#### Intuitive\n",
        "If a patient’s cholesterol is missing, replace it with the average cholesterol of similar patients.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Encoding Categorical Features\n",
        "\n",
        "#### Conceptual\n",
        "Machine learning models require numbers, not text.\n",
        "\n",
        "#### Methods\n",
        "- **Label Encoding**\n",
        "  - Male → 0, Female → 1\n",
        "- **One-Hot Encoding**\n",
        "  - Disease Type: {A, B, C} → [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "#### Intuitive\n",
        "Convert hospital form answers (Yes/No, Male/Female) into 0s and 1s.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Training the Decision Tree Model\n",
        "\n",
        "#### Conceptual\n",
        "A Decision Tree learns a set of rules that split patients into groups based on medical conditions.\n",
        "\n",
        "#### Mathematical\n",
        "Splits are chosen by maximizing Information Gain:\n",
        "\n",
        "\\[\n",
        "IG = Entropy(parent) - \\sum \\frac{n_i}{n} Entropy(child_i)\n",
        "\\]\n",
        "\n",
        "#### Intuitive\n",
        "The model asks:\n",
        "- Is age > 50?\n",
        "- Is blood pressure > 140?\n",
        "- Is glucose > 180?\n",
        "\n",
        "And finally predicts: Disease = Yes / No.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Hyperparameter Tuning\n",
        "\n",
        "#### Important Parameters\n",
        "- **max_depth** – controls tree height (prevents overfitting)\n",
        "- **min_samples_split** – minimum patients to split a node\n",
        "- **min_samples_leaf** – minimum patients in a leaf\n",
        "\n",
        "#### Method\n",
        "Use **GridSearchCV** with cross-validation.\n",
        "\n",
        "#### Mathematical\n",
        "\\[\n",
        "\\theta^* = \\arg\\min_{\\theta} \\frac{1}{k}\\sum_{i=1}^{k} Error_i(\\theta)\n",
        "\\]\n",
        "\n",
        "#### Intuitive\n",
        "Try many tree sizes and choose the one that performs best on unseen patients.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. Model Evaluation\n",
        "\n",
        "#### Metrics\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1-score\n",
        "- ROC-AUC\n",
        "\n",
        "#### Example Formula (Accuracy)\n",
        "\\[\n",
        "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "\\]\n",
        "\n",
        "#### Intuitive\n",
        "Check how many patients are correctly diagnosed as sick or healthy.\n",
        "\n",
        "---\n",
        "\n",
        "### 6. Business Value in Real-World Healthcare\n",
        "\n",
        "#### a) Early Disease Detection\n",
        "High-risk patients can be identified early, improving survival rates.\n",
        "\n",
        "#### b) Clinical Decision Support\n",
        "Doctors get an interpretable rule-based system:\n",
        "\"If BP > 140 and Sugar > 180 → High risk\"\n",
        "\n",
        "#### c) Cost Reduction\n",
        "Avoid unnecessary tests and hospitalizations.\n",
        "\n",
        "#### d) Resource Optimization\n",
        "Hospitals can prioritize critical patients.\n",
        "\n",
        "#### e) Regulatory Compliance\n",
        "Decision Trees are explainable, which is essential in medical audits.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. Summary\n",
        "\n",
        "| Step | Purpose |\n",
        "|------|--------|\n",
        "| Missing Value Handling | Avoid data loss and bias |\n",
        "| Encoding | Convert text to numbers |\n",
        "| Training | Learn diagnosis rules |\n",
        "| Hyperparameter Tuning | Improve generalization |\n",
        "| Evaluation | Ensure reliability |\n",
        "| Business Value | Better care, lower cost, trustable AI |\n",
        "\n",
        "This complete pipeline transforms raw hospital data into a reliable, interpretable disease prediction system.\n",
        "\n"
      ],
      "metadata": {
        "id": "exlHLhtXmE7E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tno0G0HXoCzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}