{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**Boosting Techniques | Assignment**"
      ],
      "metadata": {
        "id": "GFliM0uhpjiM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lz2uEt11pkWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **QUESTION 1: What is Boosting in Machine Learning? Explain how it improves weak learners.**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Conceptual Explanation**\n",
        "Boosting is an **ensemble learning technique** in Machine Learning that combines multiple **weak learners** to create a **strong learner**.\n",
        "\n",
        "A **weak learner** is a model that performs only slightly better than random guessing. Boosting works by:\n",
        "- Training models **sequentially**\n",
        "- Giving **more importance to misclassified data points**\n",
        "- Combining all models using **weighted voting or weighted averaging**\n",
        "\n",
        "The goal is to **reduce bias and error** by focusing on difficult samples.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê **Mathematical Explanation**\n",
        "In boosting, the final model is a weighted sum of weak learners:\n",
        "\n",
        "\\[\n",
        "F(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( h_m(x) \\) ‚Üí m-th weak learner  \n",
        "- \\( \\alpha_m \\) ‚Üí weight of the weak learner  \n",
        "- \\( M \\) ‚Üí total number of learners  \n",
        "\n",
        "In **AdaBoost**, weights are updated as:\n",
        "\n",
        "\\[\n",
        "\\alpha_m = \\frac{1}{2} \\ln \\left( \\frac{1 - error_m}{error_m} \\right)\n",
        "\\]\n",
        "\n",
        "Misclassified points receive **higher weights** in the next iteration.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Intuitive Explanation**\n",
        "Imagine preparing for an exam:\n",
        "- First teacher explains basics\n",
        "- Second teacher focuses on topics you got wrong\n",
        "- Third teacher concentrates only on your weak areas  \n",
        "\n",
        "Each teacher improves upon your previous mistakes.  \n",
        "Boosting works **exactly the same way**.\n",
        "\n",
        "---\n",
        "\n",
        "### üåç **Real-World Example**\n",
        "- **Spam detection**: Emails misclassified as non-spam get more focus\n",
        "- **Credit risk modeling**: Difficult-to-classify customers are reanalyzed\n",
        "- **Face recognition**: Boosting improves detection accuracy by refining errors\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Assumptions**\n",
        "- Weak learners perform better than random guessing\n",
        "- Training data contains useful patterns\n",
        "- Errors are informative for learning\n",
        "\n",
        "---\n",
        "\n",
        "### üëç **Advantages**\n",
        "- High accuracy\n",
        "- Reduces bias\n",
        "- Works well with simple models\n",
        "- Handles complex data patterns\n",
        "\n",
        "---\n",
        "\n",
        "### üëé **Disadvantages**\n",
        "- Sensitive to noisy data and outliers\n",
        "- Computationally expensive\n",
        "- Risk of overfitting if overtrained\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Use Cases**\n",
        "- Classification problems\n",
        "- Ranking systems\n",
        "- Fraud detection\n",
        "- Image recognition\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "xy5Nc-nclvl8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMqJ7qtrly2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **QUESTION 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Conceptual Explanation**\n",
        "Both **AdaBoost** and **Gradient Boosting** are **boosting ensemble techniques**, meaning they build models **sequentially**, where each new model tries to correct the mistakes of the previous ones.\n",
        "\n",
        "The **key difference lies in *how* they correct errors**:\n",
        "\n",
        "- **AdaBoost** focuses on **misclassified data points** by increasing their weights.\n",
        "- **Gradient Boosting** focuses on **reducing the overall loss** using **gradient descent**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê **Mathematical Explanation**\n",
        "\n",
        "#### üîπ AdaBoost\n",
        "AdaBoost updates **sample weights** after each iteration.\n",
        "\n",
        "Final model:\n",
        "\\[\n",
        "F(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( h_m(x) \\): weak learner\n",
        "- \\( \\alpha_m \\): weight based on error rate\n",
        "\n",
        "Weight update:\n",
        "\\[\n",
        "w_i^{(new)} = w_i^{(old)} \\cdot e^{\\alpha_m}\n",
        "\\]\n",
        "\n",
        "‚û°Ô∏è Misclassified samples get **higher weights**.\n",
        "\n",
        "---\n",
        "\n",
        "#### üîπ Gradient Boosting\n",
        "Gradient Boosting trains each model to fit the **negative gradient of the loss function**.\n",
        "\n",
        "Update rule:\n",
        "\\[\n",
        "F_m(x) = F_{m-1}(x) + \\gamma_m h_m(x)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( \\gamma_m \\): learning rate\n",
        "- \\( h_m(x) \\): model fitted on residuals\n",
        "\n",
        "‚û°Ô∏è New models learn from **residual errors**, not sample weights.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Intuitive Explanation**\n",
        "- **AdaBoost**:  \n",
        "  üëâ ‚ÄúFocus more on the questions you got wrong.‚Äù\n",
        "\n",
        "- **Gradient Boosting**:  \n",
        "  üëâ ‚ÄúReduce your total mistake score step by step.‚Äù\n",
        "\n",
        "AdaBoost targets **hard data points**, while Gradient Boosting targets **overall error reduction**.\n",
        "\n",
        "---\n",
        "\n",
        "### üåç **Real-World Example**\n",
        "- **AdaBoost**:  \n",
        "  Used in **face detection**, where difficult faces get more attention.\n",
        "\n",
        "- **Gradient Boosting**:  \n",
        "  Used in **house price prediction**, **credit scoring**, and **Kaggle competitions**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Assumptions**\n",
        "- Weak learners are simple (usually decision stumps/trees)\n",
        "- Data has meaningful patterns\n",
        "- Errors are learnable\n",
        "\n",
        "---\n",
        "\n",
        "### üëç **Advantages**\n",
        "\n",
        "**AdaBoost**\n",
        "- Simple concept\n",
        "- Effective on clean data\n",
        "- Fast convergence\n",
        "\n",
        "**Gradient Boosting**\n",
        "- Very powerful and flexible\n",
        "- Works with any loss function\n",
        "- High predictive performance\n",
        "\n",
        "---\n",
        "\n",
        "### üëé **Disadvantages**\n",
        "\n",
        "**AdaBoost**\n",
        "- Highly sensitive to outliers\n",
        "- Struggles with noisy data\n",
        "\n",
        "**Gradient Boosting**\n",
        "- Slower training\n",
        "- Requires careful tuning\n",
        "- Can overfit without regularization\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Use Cases**\n",
        "\n",
        "**AdaBoost**\n",
        "- Binary classification\n",
        "- Face detection\n",
        "- Text classification\n",
        "\n",
        "**Gradient Boosting**\n",
        "- Regression & classification\n",
        "- Financial modeling\n",
        "- Competitions (XGBoost, LightGBM)\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Summary Table**\n",
        "\n",
        "| Feature | AdaBoost | Gradient Boosting |\n",
        "|------|---------|------------------|\n",
        "| Error handling | Sample weights | Residuals (gradients) |\n",
        "| Learning method | Reweight data | Gradient descent |\n",
        "| Loss function | Exponential | Any differentiable loss |\n",
        "| Robust to noise | ‚ùå | ‚úÖ (with tuning) |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "jxsi_wFhlyNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "dygC0f1AjHLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **QUESTION 3: How does regularization help in XGBoost?**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Conceptual Explanation**\n",
        "**Regularization in XGBoost** is used to **control model complexity** and **prevent overfitting**.\n",
        "\n",
        "XGBoost extends Gradient Boosting by **adding explicit regularization terms** to the objective function.  \n",
        "These terms penalize:\n",
        "- Very complex trees\n",
        "- Large leaf weights\n",
        "- Too many splits\n",
        "\n",
        "As a result, XGBoost learns **simpler, more generalizable models**.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê **Mathematical Explanation**\n",
        "The objective function in XGBoost is:\n",
        "\n",
        "\\[\n",
        "\\text{Obj} = \\sum_{i=1}^{n} L(y_i, \\hat{y}_i) + \\sum_{k=1}^{K} \\Omega(f_k)\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(L\\) ‚Üí loss function (e.g., squared error, log loss)\n",
        "- \\(f_k\\) ‚Üí k-th decision tree\n",
        "- \\(\\Omega(f_k)\\) ‚Üí regularization term\n",
        "\n",
        "Regularization term:\n",
        "\n",
        "\\[\n",
        "\\Omega(f) = \\gamma T + \\frac{1}{2} \\lambda \\sum_{j=1}^{T} w_j^2\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\(T\\) ‚Üí number of leaves\n",
        "- \\(w_j\\) ‚Üí leaf weights\n",
        "- \\(\\gamma\\) ‚Üí penalty for adding new leaf (tree complexity)\n",
        "- \\(\\lambda\\) ‚Üí L2 regularization on leaf weights\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Intuitive Explanation**\n",
        "Think of XGBoost like writing an exam answer:\n",
        "- Writing **too much** can introduce mistakes\n",
        "- Writing **clear and concise** answers scores better\n",
        "\n",
        "Regularization forces the model to:\n",
        "- Avoid unnecessary splits\n",
        "- Keep predictions smooth\n",
        "- Focus only on important patterns\n",
        "\n",
        "---\n",
        "\n",
        "### üåç **Real-World Example**\n",
        "- **Credit risk modeling**: Prevents overfitting to rare customer behavior\n",
        "- **Stock price prediction**: Avoids reacting to market noise\n",
        "- **Medical diagnosis**: Reduces false positives caused by random variations\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Assumptions**\n",
        "- Data contains noise\n",
        "- Simpler models generalize better\n",
        "- Overfitting is possible with complex trees\n",
        "\n",
        "---\n",
        "\n",
        "### üëç **Advantages**\n",
        "- Strong overfitting control\n",
        "- Better generalization\n",
        "- Stable predictions\n",
        "- Handles noisy data effectively\n",
        "\n",
        "---\n",
        "\n",
        "### üëé **Disadvantages**\n",
        "- Requires hyperparameter tuning\n",
        "- Slightly slower training\n",
        "- Can underfit if over-regularized\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Use Cases**\n",
        "- Kaggle competitions\n",
        "- Finance and risk modeling\n",
        "- Healthcare analytics\n",
        "- Large-scale structured data\n",
        "\n",
        "---\n",
        "\n",
        "### üîß **Key Regularization Parameters in XGBoost**\n",
        "\n",
        "| Parameter | Purpose |\n",
        "|---------|--------|\n",
        "| `gamma` | Minimum loss reduction for split |\n",
        "| `lambda` | L2 regularization on leaf weights |\n",
        "| `alpha` | L1 regularization (sparsity) |\n",
        "| `max_depth` | Controls tree depth |\n",
        "| `min_child_weight` | Minimum samples in a leaf |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2sR54ZHxlviX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kimGhVSkjW93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **QUESTION 4: Why is CatBoost considered efficient for handling categorical data?**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Conceptual Explanation**\n",
        "**CatBoost** is a gradient boosting algorithm specifically designed to **handle categorical features efficiently without extensive preprocessing**.\n",
        "\n",
        "Unlike most machine learning algorithms that require:\n",
        "- One-Hot Encoding\n",
        "- Label Encoding\n",
        "- Manual feature engineering  \n",
        "\n",
        "CatBoost can **directly accept categorical variables** and process them internally in a smart and statistically robust way.\n",
        "\n",
        "Its key innovation is **Ordered Target Encoding**, which prevents data leakage and overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê **Mathematical Explanation**\n",
        "For a categorical feature \\( c \\), CatBoost replaces it with a **target-based statistic**:\n",
        "\n",
        "\\[\n",
        "\\text{Encoded}(c) = \\frac{\\sum_{i < k, c_i = c} y_i + a \\cdot P}{\\sum_{i < k, c_i = c} 1 + a}\n",
        "\\]\n",
        "\n",
        "Where:\n",
        "- \\( y_i \\) ‚Üí target value  \n",
        "- \\( P \\) ‚Üí prior (global mean of target)  \n",
        "- \\( a \\) ‚Üí smoothing parameter  \n",
        "- \\( k \\) ‚Üí current data point index  \n",
        "\n",
        "This **ordered encoding** ensures that:\n",
        "- Only **past data** is used\n",
        "- No information leakage occurs\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Intuitive Explanation**\n",
        "Imagine predicting a student's score based on their **college name**.\n",
        "\n",
        "- Using One-Hot Encoding: creates hundreds of columns  \n",
        "- Using CatBoost:  \n",
        "  üëâ Learns the *average outcome* of each college **progressively**, without seeing the future data\n",
        "\n",
        "This makes learning **faster, safer, and more accurate**.\n",
        "\n",
        "---\n",
        "\n",
        "### üåç **Real-World Example**\n",
        "- **E-commerce**: Product category, brand, seller\n",
        "- **Banking**: City, job type, account category\n",
        "- **Healthcare**: Hospital name, department\n",
        "- **Marketing**: Campaign type, customer segment\n",
        "\n",
        "CatBoost handles these categorical fields **without manual encoding**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Assumptions**\n",
        "- Categorical features influence the target\n",
        "- Dataset has mixed numerical + categorical data\n",
        "- Ordered data processing is beneficial\n",
        "\n",
        "---\n",
        "\n",
        "### üëç **Advantages**\n",
        "- No need for One-Hot Encoding\n",
        "- Prevents target leakage\n",
        "- Faster training on categorical-heavy data\n",
        "- Handles high-cardinality features well\n",
        "- Less preprocessing effort\n",
        "\n",
        "---\n",
        "\n",
        "### üëé **Disadvantages**\n",
        "- Slightly slower than LightGBM on pure numerical data\n",
        "- Model size can be large\n",
        "- Less flexible for custom loss functions\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Use Cases**\n",
        "- Kaggle competitions with categorical features\n",
        "- Customer churn prediction\n",
        "- Credit scoring\n",
        "- Recommendation systems\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Comparison with Other Algorithms**\n",
        "\n",
        "| Algorithm | Categorical Handling |\n",
        "|--------|---------------------|\n",
        "| XGBoost | Manual encoding needed |\n",
        "| LightGBM | Special categorical handling |\n",
        "| CatBoost | Native + ordered encoding |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "2aNdb1NcjZAK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vj9BkQlujmjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **QUESTION 5: What are some real-world applications where boosting techniques are preferred over bagging methods?**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Conceptual Explanation**\n",
        "**Boosting** and **Bagging** are ensemble learning techniques, but they solve **different problems**.\n",
        "\n",
        "- **Bagging** (e.g., Random Forest) mainly reduces **variance** by training models in parallel.\n",
        "- **Boosting** (e.g., AdaBoost, Gradient Boosting, XGBoost) mainly reduces **bias** by training models **sequentially**, where each model focuses on correcting the errors of the previous one.\n",
        "\n",
        "Boosting is preferred when:\n",
        "- The base model is **underfitting**\n",
        "- The data is **complex**\n",
        "- High prediction accuracy is critical\n",
        "\n",
        "---\n",
        "\n",
        "### üìê **Mathematical Explanation**\n",
        "Bagging model:\n",
        "\\[\n",
        "F(x) = \\frac{1}{M} \\sum_{m=1}^{M} h_m(x)\n",
        "\\]\n",
        "\n",
        "Boosting model:\n",
        "\\[\n",
        "F(x) = \\sum_{m=1}^{M} \\alpha_m h_m(x)\n",
        "\\]\n",
        "\n",
        "Key difference:\n",
        "- Bagging uses **equal weights**\n",
        "- Boosting uses **learned weights** and **error correction**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Intuitive Explanation**\n",
        "Think of solving a tough problem:\n",
        "\n",
        "- **Bagging**: Many students solve the same problem independently and average answers.\n",
        "- **Boosting**: Each student reviews the **mistakes** of the previous one and improves the solution.\n",
        "\n",
        "When problems are **hard and subtle**, boosting works better.\n",
        "\n",
        "---\n",
        "\n",
        "### üåç **Real-World Applications Where Boosting Is Preferred**\n",
        "\n",
        "#### üìå **1. Credit Risk & Loan Default Prediction**\n",
        "- Errors are costly\n",
        "- Boosting focuses on **hard-to-classify borrowers**\n",
        "- Used by banks and fintech companies\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå **2. Fraud Detection**\n",
        "- Fraud cases are rare and complex\n",
        "- Boosting improves detection of **minority class**\n",
        "- Bagging may miss subtle fraud patterns\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå **3. Search Engine Ranking**\n",
        "- Small ranking errors matter\n",
        "- Boosting optimizes ranking loss directly\n",
        "- Used by Google, Bing (e.g., Gradient Boosted Trees)\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå **4. Medical Diagnosis**\n",
        "- Misclassification can be dangerous\n",
        "- Boosting reduces bias and improves accuracy\n",
        "- Especially useful for disease prediction\n",
        "\n",
        "---\n",
        "\n",
        "#### üìå **5. Kaggle & ML Competitions**\n",
        "- High accuracy required\n",
        "- Boosting algorithms dominate leaderboards\n",
        "- XGBoost, LightGBM, CatBoost preferred\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **Assumptions**\n",
        "- Data has complex relationships\n",
        "- Weak learners can improve sequentially\n",
        "- Errors contain useful information\n",
        "\n",
        "---\n",
        "\n",
        "### üëç **Advantages of Boosting Over Bagging**\n",
        "- Higher predictive accuracy\n",
        "- Better bias reduction\n",
        "- Learns complex patterns\n",
        "- Focuses on difficult samples\n",
        "\n",
        "---\n",
        "\n",
        "### üëé **Disadvantages**\n",
        "- Sensitive to noise and outliers\n",
        "- Slower training (sequential)\n",
        "- Needs careful tuning\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ **Use Cases Summary**\n",
        "\n",
        "| Application | Preferred Method | Reason |\n",
        "|-----------|----------------|--------|\n",
        "| Fraud detection | Boosting | Handles rare patterns |\n",
        "| Credit scoring | Boosting | High accuracy needed |\n",
        "| Medical diagnosis | Boosting | Bias reduction |\n",
        "| Noisy data | Bagging | Variance reduction |\n",
        "| Large datasets | Bagging | Parallel training |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0D4EGLMRjnd7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uYU4zP8vjYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6: Write a Python program to:\n",
        "‚óè Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "‚óè Print the model accuracy**"
      ],
      "metadata": {
        "id": "4BIrgKP7j5BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6:\n",
        "# Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "# and print the model accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "ada_model = AdaBoostClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "ada_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = ada_model.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmWSmsBCjYht",
        "outputId": "a1b5ed19-1b99-4212-b5c2-a5c0b41b5d89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9707602339181286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iK2EMuwKjYe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 7: Write a Python program to:\n",
        "‚óè Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "‚óè Evaluate performance using R-squared score**"
      ],
      "metadata": {
        "id": "gUYFR7xlkxYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:\n",
        "# Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "# and evaluate performance using R-squared score\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJlaczMtjYcj",
        "outputId": "209e31cc-9259-4d8c-ae84-98c973f32c4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared Score: 0.7803012822391022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxRiP7DOlMN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 8: Write a Python program to:\n",
        "‚óè Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "‚óè Tune the learning rate using GridSearchCV\n",
        "‚óè Print the best parameters and accuracy**"
      ],
      "metadata": {
        "id": "YET8beTblvZl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ereyMq2Nkhgh",
        "outputId": "390a7854-da82-42ac-b20a-78b9e1a8a83c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.1, 'n_estimators': 100}\n",
            "XGBoost Classifier Accuracy: 0.9590643274853801\n"
          ]
        }
      ],
      "source": [
        "# Question 8:\n",
        "# Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "# Tune the learning rate using GridSearchCV\n",
        "# Print the best parameters and accuracy\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Define XGBoost Classifier\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Hyperparameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100]\n",
        "}\n",
        "\n",
        "# GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearch\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"XGBoost Classifier Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XSV3NRedlUjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 9: Write a Python program to:\n",
        "‚óè Train a CatBoost Classifier\n",
        "‚óè Plot the confusion matrix using seaborn**"
      ],
      "metadata": {
        "id": "PV1KjUshlUHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install catboost\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=4,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix - CatBoost Classifier\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740
        },
        "id": "A51wbM0bmgCK",
        "outputId": "31a42ff4-695d-439d-e02d-e5ab2fac561b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0PO-6t47mW-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîπ **QUESTION 10: Loan Default Prediction Using Boosting ‚Äì End-to-End Data Science Pipeline**\n",
        "\n",
        "You are working for a **FinTech company** to predict **loan default** using customer demographics and transaction behavior.  \n",
        "The dataset is **imbalanced**, has **missing values**, and contains **numeric + categorical features**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© **Step 1: Data Understanding**\n",
        "- Target variable: `Loan_Default` (0 = No default, 1 = Default)\n",
        "- Challenges:\n",
        "  - Class imbalance (defaults are rare)\n",
        "  - Missing values\n",
        "  - High-cardinality categorical features\n",
        "  - Business cost of false negatives is high\n",
        "\n",
        "---\n",
        "\n",
        "## üîß **Step 2: Data Preprocessing**\n",
        "\n",
        "### üîπ Handling Missing Values\n",
        "- **Numeric features**:\n",
        "  - Median imputation (robust to outliers)\n",
        "- **Categorical features**:\n",
        "  - Replace missing with `\"Unknown\"`\n",
        "  - Or let CatBoost handle them natively\n",
        "\n",
        "**Why?**\n",
        "- Boosting models are sensitive to noise\n",
        "- Proper imputation improves stability\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Handling Categorical Variables\n",
        "**Options:**\n",
        "- One-Hot Encoding ‚Üí High dimensionality ‚ùå\n",
        "- Target Encoding ‚Üí Risk of leakage ‚ö†Ô∏è\n",
        "- **CatBoost native handling** ‚úÖ\n",
        "\n",
        "üëâ **Preferred**: CatBoost (no manual encoding, leakage-safe)\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ Handling Class Imbalance\n",
        "- Use:\n",
        "  - `scale_pos_weight` (XGBoost)\n",
        "  - `class_weights` (CatBoost)\n",
        "- OR combine with:\n",
        "  - SMOTE (only if necessary)\n",
        "\n",
        "**Why?**\n",
        "- Defaults are rare but critical\n",
        "- Model must focus on minority class\n",
        "\n",
        "---\n",
        "\n",
        "## ü§ñ **Step 3: Model Choice (AdaBoost vs XGBoost vs CatBoost)**\n",
        "\n",
        "### ‚úÖ **Chosen Model: CatBoost**\n",
        "\n",
        "| Model | Reason for / against |\n",
        "|-----|---------------------|\n",
        "| AdaBoost | Sensitive to noise ‚ùå |\n",
        "| XGBoost | Needs encoding & careful preprocessing ‚ö†Ô∏è |\n",
        "| **CatBoost** | Best for categorical + missing data ‚úÖ |\n",
        "\n",
        "**Why CatBoost?**\n",
        "- Handles categorical features natively\n",
        "- Robust to missing values\n",
        "- Less preprocessing\n",
        "- Strong performance on tabular financial data\n",
        "\n",
        "---\n",
        "\n",
        "## üéõÔ∏è **Step 4: Hyperparameter Tuning Strategy**\n",
        "\n",
        "### üîπ Parameters to Tune\n",
        "- `iterations`\n",
        "- `learning_rate`\n",
        "- `depth`\n",
        "- `l2_leaf_reg`\n",
        "- `class_weights`\n",
        "\n",
        "### üîπ Strategy\n",
        "- Start with **default parameters**\n",
        "- Use **GridSearchCV / RandomizedSearchCV**\n",
        "- Optimize for **Recall / AUC**, not accuracy\n",
        "\n",
        "**Why not accuracy?**\n",
        "- High accuracy can ignore defaulters completely\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Step 5: Evaluation Metrics**\n",
        "\n",
        "### ‚úÖ **Primary Metrics**\n",
        "- **Recall (Sensitivity)** ‚Üí Catch maximum defaulters\n",
        "- **ROC-AUC** ‚Üí Overall discrimination power\n",
        "- **Precision-Recall AUC** ‚Üí Best for imbalanced data\n",
        "\n",
        "### ‚ùå **Avoid**\n",
        "- Accuracy (misleading for imbalanced data)\n",
        "\n",
        "| Metric | Business Meaning |\n",
        "|------|------------------|\n",
        "| Recall | Fewer bad loans |\n",
        "| Precision | Less false alarms |\n",
        "| AUC | Risk ranking quality |\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **Step 6: Model Explainability**\n",
        "- Use:\n",
        "  - Feature importance\n",
        "  - SHAP values (CatBoost supported)\n",
        "\n",
        "**Why important?**\n",
        "- Regulatory compliance\n",
        "- Trust from stakeholders\n",
        "- Explain loan rejection reasons\n",
        "\n",
        "---\n",
        "\n",
        "## üíº **Step 7: Business Benefits**\n",
        "\n",
        "### üöÄ **Business Impact**\n",
        "- Reduced loan default losses\n",
        "- Better risk-based pricing\n",
        "- Faster loan approval decisions\n",
        "- Improved customer segmentation\n",
        "\n",
        "### üí∞ **Financial Impact**\n",
        "- Lower NPA (Non-Performing Assets)\n",
        "- Higher profitability\n",
        "- Better capital allocation\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ **Final Summary**\n",
        "- Preprocess carefully (missing + imbalance)\n",
        "- Choose **CatBoost** for mixed data\n",
        "- Tune hyperparameters with business-aligned metrics\n",
        "- Evaluate using Recall & AUC\n",
        "- Deliver explainable, high-impact financial decisions\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NmFGdwmInMhy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CRP0UVADnLp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 10:\n",
        "# End-to-End Boosting Pipeline for Loan Default Prediction (FinTech Use Case)\n",
        "# Using CatBoost for imbalanced data with missing values and categorical features\n",
        "\n",
        "# ================================\n",
        "# 1. Libraries\n",
        "# ================================\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    recall_score,\n",
        "    classification_report,\n",
        "    make_scorer\n",
        ")\n",
        "\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ================================\n",
        "# 2. Sample Dataset (Simulated FinTech Data)\n",
        "# ================================\n",
        "# In real projects, this would be loaded from CSV / database\n",
        "\n",
        "np.random.seed(42) # for reproducibility\n",
        "\n",
        "num_samples = 1000\n",
        "data = pd.DataFrame({\n",
        "    \"Age\": np.random.randint(18, 70, num_samples),\n",
        "    \"Income\": np.random.randint(30000, 150000, num_samples),\n",
        "    \"Loan_Amount\": np.random.randint(5000, 50000, num_samples),\n",
        "    \"Credit_Score\": np.random.randint(300, 850, num_samples),\n",
        "    \"City\": np.random.choice([\"Mumbai\", \"Pune\", \"Delhi\", \"Bangalore\", \"Chennai\", \"Hyderabad\", None], num_samples, p=[0.2, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1]),\n",
        "    \"Employment_Type\": np.random.choice([\"Salaried\", \"Self-Employed\", \"Business\", None], num_samples, p=[0.4, 0.3, 0.2, 0.1]),\n",
        "    \"Loan_Default\": np.random.choice([0, 1], num_samples, p=[0.9, 0.1]) # Imbalanced target\n",
        "})\n",
        "\n",
        "# Introduce some missing values for numerical features\n",
        "data.loc[np.random.choice(data.index, 50, replace=False), 'Age'] = np.nan\n",
        "data.loc[np.random.choice(data.index, 70, replace=False), 'Income'] = np.nan\n",
        "\n",
        "# ================================\n",
        "# 3. Target & Features\n",
        "# ================================\n",
        "X = data.drop(\"Loan_Default\", axis=1)\n",
        "y = data[\"Loan_Default\"]\n",
        "\n",
        "# Identify categorical columns by index\n",
        "cat_features_names = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "cat_features = [X.columns.get_loc(col) for col in cat_features_names]\n",
        "\n",
        "# ================================\n",
        "# 4. Train-Test Split\n",
        "# ================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 4.1 Handle Missing Categorical Values\n",
        "# ================================\n",
        "# Fill None/NaN in categorical columns with 'Unknown' string\n",
        "for col in cat_features_names:\n",
        "    X_train[col] = X_train[col].fillna('Unknown')\n",
        "    X_test[col] = X_test[col].fillna('Unknown')\n",
        "\n",
        "# ================================\n",
        "# 5. CatBoost Model (Handles Missing + Categorical Data)\n",
        "# ================================\n",
        "# Calculate class weights for imbalance handling\n",
        "class_counts = y_train.value_counts()\n",
        "class_weights = [1, class_counts[0] / class_counts[1]] # Weight for minority class (1)\n",
        "\n",
        "cat_model = CatBoostClassifier(\n",
        "    loss_function=\"Logloss\",\n",
        "    eval_metric=\"AUC\",\n",
        "    class_weights=class_weights,\n",
        "    random_state=42,\n",
        "    verbose=0, # Suppress verbose output during grid search\n",
        "    early_stopping_rounds=50 # Add early stopping for efficiency\n",
        ")\n",
        "\n",
        "# ================================\n",
        "# 6. Hyperparameter Tuning\n",
        "# ================================\n",
        "param_grid = {\n",
        "    \"iterations\": [100, 200], # Reduced for faster execution in example\n",
        "    \"learning_rate\": [0.05, 0.1],\n",
        "    \"depth\": [4, 6]\n",
        "}\n",
        "\n",
        "# Use recall as a scoring metric, important for imbalanced data\n",
        "scorer = make_scorer(recall_score, pos_label=1) # Focus on recalling the positive class\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=cat_model,\n",
        "    param_grid=param_grid,\n",
        "    scoring=scorer,\n",
        "    cv=3, # Still use 3 folds, but with larger dataset it will work\n",
        "    n_jobs=-1 # Use all available cores\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# ================================\n",
        "# 7. Evaluation\n",
        "# ================================\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
        "print(\"Recall Score (positive class):\", recall_score(y_test, y_pred, pos_label=1))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# ================================\n",
        "# 8. Business Insight (Feature Importance)\n",
        "# ================================\n",
        "feature_importance = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": best_model.get_feature_importance()\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importance:\\n\", feature_importance)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "325c2e5c-93d6-416d-c1cf-487853b92080",
        "id": "nWVoK8HgpUKK"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'depth': 4, 'iterations': 100, 'learning_rate': 0.05}\n",
            "ROC-AUC Score: 0.48309235074626866\n",
            "Recall Score (positive class): 0.375\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.57      0.69       268\n",
            "           1       0.09      0.38      0.15        32\n",
            "\n",
            "    accuracy                           0.55       300\n",
            "   macro avg       0.49      0.47      0.42       300\n",
            "weighted avg       0.80      0.55      0.64       300\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "            Feature  Importance\n",
            "0              Age   30.249739\n",
            "5  Employment_Type   16.019515\n",
            "4             City   15.240717\n",
            "1           Income   13.188564\n",
            "3     Credit_Score   12.921577\n",
            "2      Loan_Amount   12.379887\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MNKUhGYKnFOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lO_fBCVimWto"
      }
    }
  ]
}